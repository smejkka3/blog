<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-11T17:06:37+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">My Blog</title><subtitle>My learning process and notes, including assignments from the F1Tenth 1/10 formula autonomous racing and Master Thesis Project of end-to-end autonomous parking at TU Berlin with colaboration of DAI-Labor.</subtitle><entry><title type="html">End to End Deep Learning for Autonomous Parking</title><link href="http://localhost:4000/2021/12/30/DMOSS.html" rel="alternate" type="text/html" title="End to End Deep Learning for Autonomous Parking" /><published>2021-12-30T00:00:00+01:00</published><updated>2021-12-30T00:00:00+01:00</updated><id>http://localhost:4000/2021/12/30/DMOSS</id><content type="html" xml:base="http://localhost:4000/2021/12/30/DMOSS.html"><![CDATA[<p><img src="https://www.japanautomotivedaily.com/wp-content/uploads/sites/17/2017/12/201712171206_denso-1-1024x512.jpg" alt="parking" /></p>

<p class="mycenter"><em>source <a href="https://www.japanautomotivedaily.com/2017/12/08/denso-begin-verification-testing-entry-automated-parking/">japanautomotivedaily</a></em></p>

<p>Summary of main ideas and points from some of the reading of research papers needed to implement parking system of autonomous cars using deep learining while fusing several different sensors on the car. This is project in colaboration with <a href="https://dai-labor.de/en/home/">DAI-LABOR</a> and <a href="https://be-intelli.com/">be-intelli</a> which goal is to build end to end system for autonomous parking or build deep learning system for sensor fusion.</p>

<p>One of the final goals before formulation the exact proposal of the project is to run demo og Autonomous Valet parking using <a href="https://docs.ros.org/en/foxy/index.html">ROS2</a> and <a href="https://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/">Autoware.auto</a> and <a href="https://www.svlsimulator.com">LGSVL simulator</a>.</p>

<h3 id="deep-multi-modal-object-detection-and-semantic-segmentation-for-autonomous-driving-datasets-methods-and-challenges-di-feng-et-al">Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges. Di Feng et al.</h3>
<h4 id="brief-overview">Brief overview</h4>
<p>The aim of this paper is to provide short summary of existing methods and corresponding challenges which needs to be addressed when developing autonomous driving system. It provides a short description of the most important concepts for begginers to start in the topic of autonomous driving. The main contribution is in my view their interactive online platform availible at <a href="https://boschresearch.github.io/multimodalperception/">https://boschresearch.github.io/multimodalperception/</a></p>

<h4 id="background">Background</h4>
<p>There is a high requirement for realible autonomous system as these vehicles are critical to fight with traffic congestion, and reduce current transportation carbon footprint. This is a very diffuclt task for several reasons. In order for a vehicle to operate driveless it needs to be able to perfectly percieve, predict and execute all its decision in noisy, highly unpredictible and real enviroment. Even the smallest errors can have catastrophical consequences and therefore minimizing any errors in multimodal autonomous systems is crucial.</p>

<p>The paper introduces main requirements for such a system as follow:</p>
<ol>
  <li>accurate</li>
  <li>robust</li>
  <li>real-time</li>
</ol>

<p>In next the paper is docusing on deep leraning methods for object detection and semantic segmentation, called deep multi-modal perception. The as well sumarize all topics related to developing autonomous systems such as typical automotive sensors, research purposes and some existing vehicles for testing these systems. The paper is also very useful tool of comprehensive overview of each mentioned topic and corresponding research to each of these topics.</p>

<p>There are 6 main sensing modalities in autonomous systems and each of them has its own advantages and shortcommings:</p>
<ol>
  <li>Visual and Thermal Cameras
    <ul>
      <li>Pros: provides detailed texture information, low cost</li>
      <li>Cons: sensitive to lightning and weather conditions, no depth information</li>
    </ul>
  </li>
  <li>LiDARs
    <ul>
      <li>Pros: accurate depth information, robust to lightning and weather changes</li>
      <li>Cons: no texture infrmation and sprase data points the further the object is</li>
    </ul>
  </li>
  <li>Radars
    <ul>
      <li>Pros: robust to changing lightning and weather</li>
      <li>Cons: low resolution</li>
    </ul>
  </li>
  <li>Ultrasonics
    <ul>
      <li>Pros:</li>
      <li>Cons: affected by humidity temperature and dirt,</li>
    </ul>
  </li>
  <li>GNSS and HD Maps
    <ul>
      <li>Pros:</li>
      <li>Cons:</li>
    </ul>
  </li>
  <li>IMU and Odometers
    <ul>
      <li>Pros:</li>
      <li>Cons:</li>
    </ul>
  </li>
</ol>

<p>The paper mention some of the peineering vehicles in the autonomous driving domain and it sensors with which it was able to achieve these impressive results. The first mentioned is “Boss” that won famos DARPA challenge in 2007 with camera and several Radars and LiDAR sensors. More leading companies in autonomous space are Google (Waymo), BMW, Daimler.</p>

<p>Next comes summary of main methods for perception based system. The paper speaks about deep object detection, which is a task of localization and rocognition of objects in the image. Usually these are recognized by it’s category probability and localizaed by bounding boxes. The best approach to solve object detection to date is deep learning algorithms, which set benchmarks on popular datasets such as PASCAL VOC and COCO. There are two approaches to deep object detection networks:</p>
<ol>
  <li>Two-stage detection - In first stage, regions of interest (ROI) are extracted from the image, then these candidates are verified, classified and refined based on classification scores and locations. Exaples of such a system: OverFeat, R-CNN, SPPnet, Fast-RCNN (VGG. ResNet, GoogLeNet)
    <ul>
      <li>pros: better accuracy</li>
      <li>cons: higher inference time and more time needed for training</li>
    </ul>
  </li>
  <li>One-stage detection - maps feature maps directly to bounding boxes and classification via single-stage, unified CNN model. As example authors metioned MultiBox, YOLO, SSD
    <ul>
      <li>pros: faster, easier to optimize</li>
      <li>cons: underperform in terms of accuracy to two-stage detectors</li>
    </ul>
  </li>
</ol>

<p>Authors also give good overview of availible datasets used in</p>

<h4 id="existing-datasets">Existing datasets</h4>

<h4 id="problems-in-autonomous-driving">Problems in Autonomous Driving</h4>

<h4 id="implementation">Implementation</h4>

<h4 id="challenges">Challenges</h4>

<h3 id="multi-view-3d-object-detection-network-for-autonomous-driving-xiaozhi-chen-et-al">Multi-View 3D Object Detection Network for Autonomous Driving. Xiaozhi Chen et al.</h3>
<p>todo</p>

<h3 id="joint-3d-proposal-generation-and-object-detection-from-view-aggregation-et-al">Joint 3D Proposal Generation and Object Detection from View Aggregation et al.</h3>
<p>todo</p>

<h3 id="nvidia-conference-developing-automated-parking-technologies-atousa-torabi">NVIDIA CONFERENCE: DEVELOPING AUTOMATED PARKING TECHNOLOGIES, Atousa Torabi</h3>
<p>todo</p>

<h3 id="master-general-parking-skill-via-deep-learning-y-lin">Master general parking skill via deep learning, Y. Lin</h3>
<p>todo</p>

<h3 id="assistive-parking-systems-knowledge-transfer-to-end-to-end-deep-learning-for-autonomous-parking-o-gamal-et-al">Assistive Parking Systems Knowledge Transfer to End-to-End Deep Learning for Autonomous Parking. O. Gamal et al.</h3>
<p>todo</p>

<h3 id="fisheyemultinet-real-time-multi-task-learning-architecture-for-surround-view-automated-parking-system-pullarao-maddu-et-al">FisheyeMultiNet: Real-time Multi-task Learning Architecture for Surround-view Automated Parking System. Pullarao Maddu et al.</h3>
<p>todo</p>

<h3 id="computer-vision-in-automated-parking-systems-design-implementation-and-challenges-markus-heimberger-et-al">Computer vision in automated parking systems: Design, implementation and challenges. Markus Heimberger et al.</h3>
<p>todo</p>

<h3 id="autowareauto-autonomous-valet-parking-demonstration">Autoware.Auto Autonomous Valet Parking Demonstration</h3>
<p>todo</p>

<style>
.mycenter {
    text-align:center;
}
</style>]]></content><author><name></name></author><summary type="html"><![CDATA[Notes from reference papers for project of implementing end-to-end system which performs autonomous parking using different sensors.]]></summary></entry><entry><title type="html">Joint Compatibility Branch and Bound algorithm</title><link href="http://localhost:4000/2021/04/07/JCBB.html" rel="alternate" type="text/html" title="Joint Compatibility Branch and Bound algorithm" /><published>2021-04-07T00:00:00+02:00</published><updated>2021-04-07T00:00:00+02:00</updated><id>http://localhost:4000/2021/04/07/JCBB</id><content type="html" xml:base="http://localhost:4000/2021/04/07/JCBB.html"><![CDATA[<h3 id="joint-compatibility-branch-and-bound">Joint Compatibility Branch and Bound</h3>
<p>While coarse-level data association helps us identify which clusters of LiDAR points are associated with which objects (and which ones are possibly of new objects), in order to update the state of our currently tracked objects, we need to perform point-to-point matching. Successfully performing this matching is critical to the entire system, since we use these matches to estimate a transformation that is used to update the state of our tracked objects.</p>

<p>In fine-level data association, we perform such matching. The core algorithm in this stage is the Joint Compatibility Branch and Bound (JCBB) algorithm, a common algorithm used for MOT (multi-object tracking) data association. In the JCBB algorithm, we run a depth first search to find a joint data association that minimizes jNIS (the joint normalized innovation squared).
The formula for jNIS is as follows. Say we have a set of measurements z<sub>σ</sub> that we propose are associated with a set of currently estimated boundary points h<sub>σ</sub>. The joint pairing has an innovation covariance of:</p>

<p><img src="/assets/join_paring_covariance.png" alt="join_paring_covariance" /></p>

<p>Where P is the covariance of the track, R is the expected measurement noise of the LiDAR, and Hσ is the Jacobian of the measurement model of the boundary points. Then, the jNIS is:</p>

<p>jNIS = (z<sub>σ</sub> - h<sub>σ</sub>)<sup>T</sup> S<sup>-1</sup><sub>σ</sub>(z<sub>σ</sub> - h<sub>σ</sub>)</p>

<p>This formula is similar to the <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis distance</a>, which measures the distance between two probability distributions.</p>

<h4 id="implementation">Implementation</h4>
<p>The JCBB fine association algorithm is comprised of three parts. First, we construct an initial association based on the output of coarse association– scan points and estimated boundary points that are sent into an array to form our initial association.
Individual Association: For every scan point, and every estimated boundary point, we then compute their potential individual compatibility. This allows the later Depth First Search to operate efficiently, by pre-rejecting incompatible points (typically ones that are far away from each other). The initial association is then pruned based on these individual compatibilities.
Prune Association: We also wish that our output association has a one-to-one pairing between bound- ary points and scan points. That is, each scan point can only be paired to at once one boundary point, and each boundary point can only be paired to at once one scan point. Thus, the initial association is further refined to remove double-pairing – in cases where two scan points are paired to one boundary point (or vice versa), we simply clear the association for both points.</p>

<p>Minimal Association: Then, we run through the association, and iteratively remove pairs from the association to yield the largest jNIS decrease. We continue doing this until the total jNIS of the association falls below the chi squared significance value. In practice, this operation is computationally expensive (it requires lots of iterations), so we cut this portion of the algorithm off if the number of iterations exceeds a max iteration parameter.</p>

<h5 id="depth-first-search">Depth First Search:</h5>
<p>With our minimal association, we begin adding points to the association to generate a valid association (one whose jNIS falls below the chi squared significance value) with the maximum number of pairs. If there are multiple associations with the same number of pairs, we choose the one with the lowest jNIS value.
In practice, we implement this portion of the algorithm as a depth first search. A diagram of a sample search tree is shown below. As can be seen, each level of the tree refers to a scan point – as we go through the tree, we search through sets of boundary points that are associated with our scan points. Every level of the tree has an option for its scan point to be associated with no boundary points, in which case it becomes associated with a ”nAn” (the red circles in the below plot). At every point in the tree, we recalculate the jNIS of the proposed association thus far.</p>

<p><img src="/assets/tree_structure.png" alt="tree_structure" /></p>

<p>To improve the efficiency of the DFS search, we prune portions of the tree. jNIS is a monotonically increasing function – as we increase the number of pairs in our association, the jNIS will always increase. As such, if, while going down a branch of the tree, we see that our jNIS is higher than the chi squared significance value, we cut off the remainder of the branch.
We further prune our search tree by maintaining the best jNIS and highest number of associations as class variables. As such, once again, if we see that our jNIS is higher than the best recorded jNIS, we cut off the search of that particular branch.
Polar to Cartesian: The original paper recommends calculating jNIS based on polar coordinates; as such, the distance between points is measured as the distance between their r and θ values in a polar graph. However, we find that calculating jNIS in Cartesian coordinates yields far better associations, due to the fact that when the measured points are far away from the ego vehicle, differences that seem small in Polar space are actually large in Cartesian space. This is shown in the following figure.</p>

<p><img src="/assets/jcbb_matches.png" alt="jcbb_matches" /></p>

<h5 id="speed-improvements">Speed Improvements:</h5>
<p>The JCBB is the slowest part of the system by far. As such, we experimented with a number of methods to increase its speed.
In practice, the DFS is the most time-consuming part of JCBB, as the search space of the DFS (even with pruning) can increasing dramatically if the number of scan points or boundary points is high. As such, we limit the number of recursions through the DFS– if we hit this limit, we simply return the best association generated thus far.
One of the most time-consuming parts of the DFS is the repeated recalculation of the jNIS. This recal- culation involves the inversion of the covariance matrix Sσ, which can get computationally expensive if Sσ is large.
To help speed up this calculation, we take advantage of the fact that the jNIS can be calculated in triangular form. Instead of inverting Sσ each time we run through the DFS, we instead invert a covariance matrix S with all the boundary points and all the scan points at the very beginning of the algorithm (during the individual compatibility step). Then, we take its cholesky and store it as a class variable.
Sσ can be approximated by taking individual rows and columns from the stored cholesky matrix. We can then solve for the jNIS via the following algorithm:</p>

<p><img src="/assets/jnis.png" alt="jnis" /></p>

<p>While the resulting jNIS is not completely accurate, as the inverse of portions of the matrix S cannot be completely accurately determined by taking rows and columns from its cholesky, we find that the speed increase (around 10x) yielded by using this approximation exceeds the resulting loss of accuracy.
To further improve the speed of JCBB, we wrote a version of it using Numba. We find that this sped up the implementation by around 1.5x ˆahowever, on certain frames, the algorithm can slow down quite a bit. Below is a plot of the loop run times taken from the two systems (the non-Numba JCBB, and the numba JCBB):</p>

<p><img src="/assets/jcbb_runtime.png" alt="jcbb_runtime" /></p>

<p>Lastly, we attempted to remove the JCBB entirely from the algorithm, by replacing it with a simple algorithm that minimizes the joint distance between two sets of points, and rejects pairings that are too far apart. In practice, we find that this implementation does not generate good associations, causing rapid divergence in the algorithm output.</p>

<h3 id="transformation-estimation">Transformation Estimation</h3>
<p>Fine association outputs a set of paired scan points and currently estimated boundary points. These pairs are used to update the hidden state of our tracks.
To do this, we diverge from the paper, and use these pair points to estimate a transformation T between the estimated boundary points and scan points. Given that we know the timestep between observations, we can use T to create a fictious measurement of the track’s position, orientation, and velocity.
A rigid body transformation is appropriate in this use-case, as we are designing this system for F1Tenth racing, where the only tracks we will likely encounter are other cars, walls, or cones, all relatively rigid objects. This assumption, however, may not be appropriate if this system were deployed on a vehicle that may observe non-rigid objects like humans or animals.
As we are estimating a 2D rigid body transformation, T will simply be a 3x3 matrix that encodes the SE(2) transformation that best maps our scan points to our estimated boundary points. To best estimate this transformation, we can either estimate a least squares solution to calculate the transformation matrix between the entire set of pairs via the Kabsch algorithm, or as a cheaper alternative which performs better when the number of pairs is high, run RANSAC, and compute the transformation matrix for a minibatch of pairs (via Kabsch), keeping the transformation that generates the highest number of inliers on the entire set of pairs.</p>

<p><img src="/assets/ransac.png" alt="RANSAC" /></p>

<p>Once the transformation matrix is estimated, it can be used to approximate the fictious measurement by assuming that the transformation occurred as the track was moving with a constant linear and angular velocity.</p>

<p><img src="/assets/transformation_estimation.png" alt="transformation_estimation" /></p>

<h3 id="measurement-model">Measurement Model</h3>
<p>Our “fictious” measurements can be thought of direct observations of the position, velocity, and orientation of tracked objects. These measurements, however, are quite noisy, as data association and transformation estimation both are inherently noisy processes. As such, our measurement model can be thought of as following the below equation, where R is a diagonal noise covariance matrix. In particular, the terms in R associated with velocity (angular and linear) have noise terms that are larger than those associated with position, as velocity estimation is likely to be more noisy that position estimation.</p>

<p><img src="/assets/measurement_model.png" alt="measurement_model" /></p>

<p>A more robust treatment of the problem may involve looking at the correlation between velocity and position measurements, thus making placing off-diagonal elements in R. Experimentally, however, we find that treating R as a diagonal matrix generates sufficiently accurate tracking.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post we are going to have a close look at Joint Compatibility Branch and Bound (JCBB) as it's important part of the implementation of fine association of the paper Model-Free Detection and Tracking of Dynamic Objects with 2D LiDAR by Dominic Zeng Wang]]></summary></entry><entry><title type="html">Iterative closest point algorithm</title><link href="http://localhost:4000/2021/03/05/ICP.html" rel="alternate" type="text/html" title="Iterative closest point algorithm" /><published>2021-03-05T00:00:00+01:00</published><updated>2021-03-05T00:00:00+01:00</updated><id>http://localhost:4000/2021/03/05/ICP</id><content type="html" xml:base="http://localhost:4000/2021/03/05/ICP.html"><![CDATA[<h3 id="coarse-association-and-icp">Coarse Association and ICP</h3>
<p>As already mentioned, the purpose of coarse association is for algorithm to get and idea which objects in the current LiDAR scan belongs to already existing objects and static background. It is based on modified version <a href="https://ieeexplore.ieee.org/document/121791">Iterative Closest Point Algorithm (ICP)</a>, which we are going to describe more closely in this section.
ICP is efficient algorithm for minimizing the difference between two clusters of points. In robotics it is mainly used for reconstructing 2D or 3D surfaces from different scans, to localize robots and achieve optimal path planning. In our work we use it to align two clusters within certain distance threshold and compute rough guess of the rigid transformation between target and source clusters. If both translation and rotation between these two clusters are lower than very small, fixed threshold within multiple iteration, we say the the target cluster is corresponding to the source cluster. ICP can be very slow if we have big initial misalignment between target and source, however with scanning frequency of out LiDAR 40Hz we are making sure that the distance and misalignment between two objects can’t be so high that ICP would fail to converge quickly, given our prediction model is correct as the ICP is run right after the prediction step. <a href="https://www.robots.ox.ac.uk/~mobile/Papers/2015IJRR_wang.pdf">Wang et. al</a> recommends to estimate association validity based on an ICP procedure that involves a nearest neighbor estimation. This simple nearest neighbor procedure means that for many shapes, all points in one object are associated with a single point in another. This yields bad transformation estimation. This can be seen on the following figure.</p>

<p><img src="/assets/icp_problem.png" alt="icp_problem" /></p>

<p>The problem using nearest neighbour in ICP, as it can generate 1-n association of points, which leads to bad transformation estimation between objects. In this case point 106 in the blue object is assigned as the closest point to every point in the green object, this leads to wrong calculation of transformation between these 2 objects.</p>

<h4 id="changes-to-original-icp">Changes to original ICP</h4>
<p>We propose a new estimation based on minimizing the total distance between a group of pairs, where we make sure that if we assign point from one object to point in another object, these points are not considered again as candidates for another point pair. Using this approach, we have seen significant improvement compared to the [5] not only in terms of precision, but speed of convergence of ICP as well.</p>

<p>With this version of ICP, we align the incoming lidar scan with clusters which already has been associated to static background. After that, we calculate a transformation between the corresponding points: if the transformation (either its translation magnitude or rotation magnitude) is too large, then the association is rejected and the cluster deemed to not be a good match with the candidate object.</p>

<p>The resulting transformation between found point pairs from target and source cluster is done by scikit-image library using Euclidean Transformation. The result of our associations computed by ICP on clusters from figure 4 is shown figures 5.
<img src="/assets/coarse_association_results.png" alt="coarse_association_results" /></p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post we are going to have a close look at iterative closest point algorithm (ICP) as it's important part of the implementation of the paper Model-Free Detection and Tracking of Dynamic Objects with 2D LiDAR by Dominic Zeng Wang]]></summary></entry><entry><title type="html">Reading notes: Model-free detection and tracking of dynamic objects with 2D lidar, Wang et al.</title><link href="http://localhost:4000/2021/02/12/Reading-Wang-et-al.html" rel="alternate" type="text/html" title="Reading notes: Model-free detection and tracking of dynamic objects with 2D lidar, Wang et al." /><published>2021-02-12T00:00:00+01:00</published><updated>2021-02-12T00:00:00+01:00</updated><id>http://localhost:4000/2021/02/12/Reading-Wang-et-al</id><content type="html" xml:base="http://localhost:4000/2021/02/12/Reading-Wang-et-al.html"><![CDATA[<p>Mentioned in post before, the goal of the project is to implement algorithm mentioned in the paper from <a href="https://www.robots.ox.ac.uk/~mobile/Papers/2015IJRR_wang.pdf">Wang et. al</a>. The paper focuses on creating framework that estimates and determines what is static background as well as movement of dynamic objects.</p>

<p>The algorithm itself can be divided into several parts. Firstly the new laser and odometry measurements coming from car need to be processed. The process of laser measurement consists of first cleaning the states which is mentioned very briefly in the paper and from my understanding, this means that all points from the previous measurement which are no longer in the LiDAR’s field of view, should be dropped. The authors do not provide any reasoning for this but my guess is that it’s due the easing some of the memory requirement as we do not essentially need to work with anything which we can’t see. One of our own modification during cleaning the states is also drop all points in the memory which are beyond certain threshold. This threshold is chosen to maximise the distance to which we want to detect dynamic object, but at the same time minimise the memory requirement as the assumption at the moment is that the whole framework needs to run in real-time to be usable, which can be hard to achieve with too many “memorised” points. This process is followed by forward propagation, where the motion part of all dynamic tracks according to an appropriate motion model is predicted. This motion model is required, due the main objective of the algorithm, to not have any assumption about the object class (therefore it’s expected motion pattern). This steps is followed by association and update of dynamic and static object measurements finalised by merging tracks which correspond to the same class (static or specific dynamic track). The basic idea of the algorithm is outlined in following figure
<img src="/assets/wangetal.png" alt="alg" /></p>

<p>As mentioned already the main benefit of this approach is that it allows us to track objects without having to know any informations about the object beforehand. Let’s describe each step more into the detail to understand more details about the algorithm.</p>

<h3 id="sensor-pose-prediction-on-odometry-measurement">Sensor pose prediction on odometry measurement</h3>
<p>Individual laser scan beams and a set of odometry measurements are used as input to the algorithm. Without odometry information, the system would not be able to distinguish static objects as all motion estimates are relative to the LiDAR sensor. Odometry information provides us with an estimate of speed v of non-holonomic vehicle, mean angle θ of front wheels.</p>

<h3 id="dynamic-object-motion-prediction">Dynamic object motion prediction</h3>
<p>When new measurements are received, currently existing dynamic tracks are predicted forward using generic motion prediction model before they are updated with the newly received scans. We assume a constant velocity model, because we do not know any information about the object and thus it’s motion pattern. This is disadvantage of working with model-free assumption. We also model not only constant linear velocity component, but angular velocity component as well. This should make motion prediction model able to capture a full range of 2D rigid body motions.</p>

<h3 id="hierarchical-data-association">Hierarchical data association</h3>
<p>It is not possible to directly observe all state variables in the joint state and for those which we can directly observe, more specifically boundary points belonging to static or dynamic object, it is difficult to tell which one do they belong to or if it is a new boundary point, which haven’t been seen before. Each time we get new set of LiDAR scan, we have to determine following:</p>
<ol>
  <li>If new measurement is a part static object:
    <ul>
      <li>Is it already existing boundary point?</li>
      <li>Is it a new boundary point?</li>
    </ul>
  </li>
  <li>If new measurement is a part of existing dynamic object:
    <ul>
      <li>Is it already existing boundary point on the object? If that’s the case it also needs to be determined which specific object this measurement belongs to.</li>
      <li>Is it a new boundary point on the object?</li>
    </ul>
  </li>
  <li>Is new measurement part of new, yet unassociated object?</li>
</ol>

<p>In order to solve data association problem we have to associate the measurements on low and high level. Meaning, on high level we want to firstly create rough associations on coarse scale, where we divide all newly observed measurements into clusters and find out for each cluster if it belongs to already existing static background or dynamic object. If neither is the case than we assign it as a new, yet unobserved object. On low level we create fine associations, where the each measurements within each cluster is either associated to yet exiting boundary point or used to initialized a new point.</p>

<h4 id="coarse-level-data-association">Coarse-level data association</h4>
<p>In the previous section we discussed that in order to decide if the new set of measurements belongs to boundaries of already existing objects or if the measurements are new objects, we need to break down the problem into coarse and fine level association. We mentioned as well that in coarse association we want to divide incoming measurements into clusters. More specifically, we divide incoming laser scan data into set of clusters C = {C<sub>1</sub>, C<sub>2</sub>, …, C<sub>|C|</sub>}.</p>

<p>Using ICP, we align the incoming lidar scan with clusters which already has been associated to static background. After that, we calculate a transformation between the corresponding points: if the transformation (either its translation magnitude or rotation magnitude) is too large, then the association is rejected and the cluster deemed to not be a good match with the candidate object. Associated clusters that are assigned static background are removed from the set of clusters C. Then, we repeat the same procedure on the remaining clusters in C, however this time we are trying to match incoming clusters with already known dynamic objects using ICP.</p>

<p>The rough coarse association algorithm is visualised in pseudocode bellow.
<img src="/assets/coarse_association_alg.png" alt="coarse_association_alg" /></p>

<h4 id="fine-level-data-association">Fine-level data association</h4>
<p>While coarse-level data association helps us identify which clusters of LiDAR points are associated with which objects (and which ones are possibly of new objects), in order to update the state of our currently tracked objects, we need to perform point-to-point matching. Successfully performing this matching is critical to the entire system, since we use these matches to estimate a transformation that is used to update the state of our tracked objects.</p>

<h3 id="track-initialization-and-merging">Track initialization and merging</h3>
<p>During each LiDAR update iteration, we check to determine whether or not any currently tracked object is either part of the static background, or part of another dynamic object. In this, we diverge from original paper and check all objects, rather than just the ones newly initialized. We find experimentally that this allows the algorithm to correct previous mistakes by merging tracks that were previously, and mistakenly, not merged.
To perform the merging step, we calculate the Mahalanobis distance between each track’s state and the merging target’s state, taking into account the track’s covariance. We first test to see whether or not each track should be merged with the static background by comparing the track’s velocity with a proposed value of 0 (static objects have a velocity of 0). To determine whether or not a track is part of the static background, we compute the Mahalanobis distance between the track’s velocity probability distribution and 0, and compare the distance against a chi squared significance value. If the distance is lower than the chi squared value, then we merge the track with the static background.
The remaining tracks are then tested against other dynamic tracks to see whether or not they should be merged. The merging procedure works here nearly the same as the procedure for the static background: rather than comparing the track’s velocity, however, we compare the track’s position and velocity against the proposed merge target’s. If the calculated Mahalanobis distance between these two distributions falls below the chi squared significance value, then these tracks are merged.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Notes from paper for Model-Free Detection and Tracking of Dynamic Objects with 2D LiDAR by Dominic Zeng Wang, Ingmar Posner and Paul Newman.]]></summary></entry><entry><title type="html">F1Tenth Project - Object Tracking</title><link href="http://localhost:4000/2021/02/10/F1Tenth_Project.html" rel="alternate" type="text/html" title="F1Tenth Project - Object Tracking" /><published>2021-02-10T00:00:00+01:00</published><updated>2021-02-10T00:00:00+01:00</updated><id>http://localhost:4000/2021/02/10/F1Tenth_Project</id><content type="html" xml:base="http://localhost:4000/2021/02/10/F1Tenth_Project.html"><![CDATA[<p>The project I’m going to be working on is object tracking using LiDAR initially. The algorithm is based on the paper <a href="https://www.robots.ox.ac.uk/~mobile/Papers/2015IJRR_wang.pdf">Wang et. al</a> where the main goal is to implement solution for detection and tracking of moving object using 2D laser scanner.</p>

<h2 id="description-of-tasks">Description of tasks</h2>
<p>Robust object tracking and localization is a critical need for autonomous cars, allowing for safe,
intelligent navigation and planning. Cars must be able to perform this task rapidly and accurately,
requiring algorithms with high output frequencies and low false-negative and false positive rates.
While much work has been done on object tracking and localization with full-scale cars, the F1Tenth
Object Tracking project seeks to implement tracking on the F1Tenth autonomous race
car system.</p>

<p>Initially, this project will build on the work of Dominic Zeng Wang, who proposed a
LiDAR-based object tracking system that uses the knowledge of the static local background to help identifying dynamic
objects from static ones in a principled and straightforward way. This project will
implement and test his proposed system on the F1Tenth vehicle, making use of its onboard 2D
LiDAR. Later, I will combine LiDAR-based tracking with visual tracking, in order to increase
the robustness of the system by increasing the signal-noise ratio from the vehicle’s sensors.
Given the restrictions posed by the pandemic, this project will heavily leverage virtual
environments like F1Tenth team’s proprietary gym environment. If possible, we will
attempt to implement the system on the actual F1Tenth car for further real-life testing and
validation.</p>

<h2 id="step-by-step-to-dos">Step by Step to Dos</h2>
<h3 id="phase-1">Phase 1:</h3>
<p>Get Familiar with F1Tenth – January 25 - February 3</p>
<ul>
  <li>Learn about the F1Tenth car, the F1Tenth System, and the F1Tenth Simulator.</li>
  <li>Learn about the ROS framework.</li>
  <li>Learn about F1Tenth fundamentals with the labs linked here.</li>
</ul>

<h3 id="phase-2-get-familiar-with-fundamentals">Phase 2: Get Familiar with Fundamentals</h3>
<h5 id="february-3--february-14">February 3 – February 14</h5>
<ul>
  <li>Carefully study Wang paper, along with related works.</li>
  <li>Learn how to use F1Tenth Gym environment.</li>
  <li>Get familiar with fundamentals of Bayes filtering.</li>
  <li>Write progress report / documentation as needed.</li>
</ul>

<h3 id="phase-3-implementation">Phase 3: Implementation</h3>
<h4 id="february-14--march-14">February 14 – March 14</h4>
<ul>
  <li>Implement virtual sensing system specified in Wang paper on F1Tenth.</li>
  <li>Implement association algorithms for LiDAR data, divide it into meaningful pieces, and associate those pieces with tracked vehicles on F1Tenth.</li>
  <li>Identify areas where visual data can increase robustness of LiDAR system.</li>
  <li>Write progress report / documentation as needed.</li>
</ul>

<h3 id="phase-4-expansion">Phase 4: Expansion</h3>
<h5 id="march-14--april-11">March 14 – April 11</h5>
<ul>
  <li>Develop pipeline to validate LiDAR tracking hypothesis with camera data, and camera
hypothesis with LiDAR data</li>
  <li>Write progress report / documentation as needed.</li>
</ul>

<h3 id="phase-3-wrapping-up-phase">Phase 3: Wrapping up Phase</h3>
<h5 id="april-11--may-10">April 11 – May 10</h5>
<ul>
  <li>Continue working on the previous goals that have not been accomplished.</li>
  <li>Documentation for future development (user guide / issues / achievements).</li>
  <li>Optional: Demonstrate to potential business partners / investors.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Chose a project to work on in F1Tenth which is a object tracking on the F1Tenth autonomous race- car system and implement and test the proposed system on the F1Tenth vehicle, making use of its onboard 2D LiDAR.]]></summary></entry><entry><title type="html">Pure Pursuit</title><link href="http://localhost:4000/2020/11/19/pure-pursuit.html" rel="alternate" type="text/html" title="Pure Pursuit" /><published>2020-11-19T00:00:00+01:00</published><updated>2020-11-19T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/19/pure-pursuit</id><content type="html" xml:base="http://localhost:4000/2020/11/19/pure-pursuit.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Very effective algorithm all the way from 1992 is based on very simple idea of following a waypoint. It is tracking algorithm with assumption that we already know where the set of waypoints is known both in global and local coordinate. This can be done by cartographer SLAM algorithm for example.</p>

<h3 id="geometric-representation">Geometric representation</h3>
<p>Starting from car’s local frame (base_link), we need to define so called Lookahead distance L, which is simply radius around the base_link where we are looking for waypoint. Because F1Tenth is nonholonomic robot, to get to the waypoint we need to turn along some arc, it is not possible to move to the waypoint in straight direction. However there can be indefinitely many arcs define to a single point as sown on the figure bellow (ref <a href="https://f1tenth.org/learn.html">f1tenth Module D, lecture 13</a>).
<img src="/assets/pure_pursuit_geometric.png" alt="pp" />
To solve this, we need to constrain the centre of the arc to be on y axes of the car. Using this we can start solving the equations bellow using basic trigonometry. Because we constrained the centre on y axis as radius of the arc, r can be computed as r = |y| + d. We also have right angle triangle therefore d<sup>2</sup> + x<sup>2</sup> = r<sup>2</sup>. Substituting d as the first equation r = |y| + d we get (r - |y|)<sup>2</sup> + x<sup>2</sup> = r<sup>2</sup> and solving this equation we get r<sup>2</sup> + L<sup>2</sup> - 2r|y| = r<sup>2</sup>. The L is again from simple Pythagorean theorem because we know x<sup>2</sup> and y<sup>2</sup> in the triangle we can express L as L<sup>2</sup> = x<sup>2</sup> + y<sup>2</sup> shown on the picture bellow (ref <a href="https://f1tenth.org/learn.html">f1tenth Module D, lecture 13</a>)
<img src="/assets/pp_geometric2.png" alt="pp2" /></p>

<p>Solving r<sup>2</sup> + L<sup>2</sup> - 2ry = r<sup>2</sup> for r we have r = L<sup>2</sup>/2y. Since we know the radius, we can compute the curvature of arc as inverse of radius γ = 1/r = 2y/L<sup>2</sup></p>

<h3 id="picking-a-goal-point">Picking a goal point</h3>
<p>There is many choices of how to pick a waypoint in the lookahead radius L, the simplest one is probably either choice the farthest waypoint the the L circle or closest outside of circle. Another example could be get both the closest point outside of circle and farthest point inside of circle, connect them and set a waypoint to the intersection of this connection and circle of lookahead distance. Visualised on figure bellow (ref <a href="https://f1tenth.org/learn.html">f1tenth Module D, lecture 13</a>)</p>

<p><img src="/assets/pp_waypoint.png" alt="waypoint" /></p>

<h3 id="tuning">Tuning</h3>
<p>The main thing we need to is the lookahead distance L. Smaller L acts same as higher Kp in PID control where the car is gonna be more aggressive and do higher curvatures and oscillations, with higher L is the opposite, the trajectory will be smoother, but the tracking error will be higher.</p>

<h3 id="pipeline">Pipeline</h3>
<p>(ref <a href="https://f1tenth.org/learn.html">f1tenth Module D, lecture 13</a>)</p>
<ol>
  <li>Create a map using Cartographer</li>
  <li>Create a set of waypoints using global planner (e.g. by driving by teleop)</li>
  <li>Pick waypoints to track at each frame</li>
  <li>Set steering angle to track current waypoint</li>
  <li>Update the waypoint on the go</li>
</ol>

<p>The original paper can be found at <a href="https://www.ri.cmu.edu/pub_files/pub3/coulter_r_craig_1992_1/coulter_r_craig_1992_1.pdf">Pure Pursuit implementation</a>, it’s short and well worth reading for understanding this algorithm.</p>

<h2 id="f1tenth-lab6-assignment">F1Tenth Lab6 assignment</h2>

<h3 id="encountered-difficulties">Encountered difficulties</h3>
<p>During this assignment for the first time I experienced some longer sessions on front of monitor to resolve some of the issues I had, as well as had problems with my programmings skills themselves.</p>

<ol>
  <li>Cartographer:
I was able to make cartographer work locally as shown on examples on <a href="https://google-cartographer-ros.readthedocs.io/en/latest/demos.html">https://google-cartographer-ros.readthedocs.io/en/latest/demos.html</a>, after that I moved the files as mentioned in the <a href="https://youtu.be/L51S2RVu-zc?t=4403">pure_pursuit lecture video</a>, I had to change in file F110.xarco line 194 from
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> &lt;mesh <span class="nv">filename</span><span class="o">=</span><span class="s2">"package://racecar_description/meshes/hokuyo.dae"</span>/&gt;
</code></pre></div>    </div>
    <p>to</p>
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> &lt;mesh <span class="nv">filename</span><span class="o">=</span><span class="s2">"package://f110_description/meshes/hokuyo.dae"</span>/&gt;
</code></pre></div>    </div>
    <p>From this I suppose that this should be only related to the real car, as I’m not able to make the model of the car work in the simulation of RVIZ, where RVIZ have problem transforming some parameters related to the car. So instead of running cartographer in unknown map I used one of the map that was saved in f1tenth_simulator package already and run waylogger in this map.</p>
  </li>
  <li>Switching from Python 3 to Python2.7:
In order to run waylogger I needed to install package with Particle Filter. For this I used <a href="https://github.com/mit-racecar/particle_filter">MIT RACE CAR ROS PACKAGE</a> as I didn’t find this package anywhere in the f110. Up until this point I was using python3 without any problems in the assignments so far however this particle_filter and waylogger package both were using python2.7 so I decided to switch my ROS melodic to python2. However setting .bashrc file to python 2.7 didn’t switch my ROS system to python2 and was still using python3. I spent long hours trying to figure this out because I think as I was using anaconda with python3 there were some settings which prevented the any system in my ubuntu from using python2 and even completely reinstalling whole ROS melodic didn’t and removing anaconda reference from .bashrc file didn’t work. I had to uninstall the anaconda completely as well as uninstall and install back ROS.</li>
  <li>Waypoint Logger:
After reinstalling my whole ROS the waylogger was finally working, however it needed running particle_filter topic otherwise it didn’t log anything. In the above mentioned particle filter for MIT car I had to change <a href="https://github.com/mit-racecar/particle_filter/blob/master/launch/localize.launch">localize.launch</a>, concretely comment line 30
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> 	&lt;<span class="o">!</span><span class="nt">--</span> &lt;include <span class="nv">file</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>find particle_filter<span class="si">)</span><span class="s2">/launch/map_server.launch"</span>/&gt; <span class="nt">--</span><span class="o">&gt;</span>
</code></pre></div>    </div>
    <p>as I was using my own map server for the map from f1tenth_simulator package
and in line 33 use only /odom topic</p>
    <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> 	&lt;arg <span class="nv">name</span><span class="o">=</span><span class="s2">"odometry_topic"</span> <span class="nv">default</span><span class="o">=</span><span class="s2">"/odom"</span>/&gt;
</code></pre></div>    </div>
  </li>
  <li>Launching waylogger and particle_filter:
Strangely enough to get correct coordinates in logfile the particle_filter node needs to be launched as first, before the map and rviz are launched, otherwise the resulting coordinate system is different than the map launched.</li>
</ol>

<p>After all of this above I was able to generate proper logfile with right coordinates.</p>

<p>Probably most difficulties during the programming of assignment was to correctly transform the global and car correctly otherwise the rest was very straight forward using the formulas from lecture. I expected to have more problems with the visualization, however the ROS Marker tutorial is easy to follow and the visualization was also quickly implement.</p>

<iframe width="800" height="400" src="https://www.youtube.com/embed/sAk8qmBD_LI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<iframe width="800" height="400" src="https://www.youtube.com/embed/aFKKos5si0Y" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>]]></content><author><name></name></author><summary type="html"><![CDATA[First practical experience with Cartographer SLAM package and implementation of trajectory planner called the Pure Pursuit algorithm.]]></summary></entry><entry><title type="html">Scan matching</title><link href="http://localhost:4000/2020/11/14/scan_matching.html" rel="alternate" type="text/html" title="Scan matching" /><published>2020-11-14T00:00:00+01:00</published><updated>2020-11-14T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/14/scan_matching</id><content type="html" xml:base="http://localhost:4000/2020/11/14/scan_matching.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Last post was about SLAM, where I introduced approach to localization, here we are going to look at approach to lacalization using sets of range measurements collected over across time. Going back, localization is state of robot with respect to the environment it find self in. One approach to localization is to use odometry, basically estimating the current pose of the robot from knowing a start position and integration of control and motion measurements. However there is accumulation of very small errors in the position of the robot which becomes issue over some extended period time and the uncertainty of the pose increases. One way to solve this is to use range sensor measurements to localize, also known as scan matching.</p>

<h2 id="setting-a-problem">Setting a Problem</h2>
<p>Having a scan from robot of measurements of distances to some landmarks A,B,C in some environment, we call this measurements at t=0 distances in local frame of reference. When we move in unknown direction at time t=1, the distances are obviously going to change. We can measure these distances at time t=1 and we want find transform R which transforms the two set of points to be the closest and this transform represents how much the robot moved. This is visualised on the picture bellow for better intuition ( ref <a href="https://f1tenth.org/learn.html">module C, lecture 9 F1Tenth website</a>)
<img src="/assets/problem_scan_matching.png" alt="problem_scan_matching" /></p>

<p>However only from scan readings alone, we can’t know the landmarks A,B,C exactly, therefore we have to assume that the closest points correspond to each other(correspondence match) and than iteratively find the best transform R.</p>

<h3 id="iterative-search-for-the-best-transform">Iterative search for the best transform</h3>
<ol>
  <li>Make a initial guess of R</li>
  <li>For each point in new scan (t=k+1) find closest point in previous set (t=k) -&gt; correspondence search</li>
  <li>Make another better guess of R</li>
  <li>Set next guess to the current guess (repeat 2-4 until converges)</li>
</ol>

<p>For this algorithm (Iterative closest point or ICP) to converge, the choice of initial guess is important. Also ICP can be slow. For this reason PL-ICP was introduced (Point-to-Line Iterative Closest Point).</p>

<h3 id="point-to-line-iterative-closest-point">Point-to-Line Iterative Closest Point</h3>
<p>Instead of looking for point to point metric, PL-ICP is looking for point-to-line metric. Point-to-point measures the distances the distance to the nearest point on the segment, while point-to-line measures distance to nearest line containing the segment. Point-to-Line converges faster because the level sets the approximate surface better and therefore results in more accurate approximation or the error. This is achieved by the contours of the level being line and not concentric and centred around the projected point and thus giving algorithm quadratic convergence instead of linear convergence.</p>

<h4 id="pl-icl-algorithm">PL-ICL algorithm</h4>
<p>at t=k</p>
<ol>
  <li>use previous guess q_k, transform coordinates of current scan into frame of previous scan</li>
  <li>For each point, find closest line segment (correspondence search)</li>
  <li>Update transform
  a. formulate point-to-line-error objective
  b. find transform q<sub>k+1</sub> that minimizes objective</li>
</ol>

<p>PL-ICL also uses smart “tricks” in correspondence match search. It does local search with early termination and jump table for each scan point.</p>

<p>TODO DO MORE RESEARCH IN PL-ICL AND DESCRIBE BETTER POSSIBLY AFTER ASSIGNMENT.</p>

<p>To understand this algorithm better I recommend read trough original paper <a href="https://censi.science/pub/research/2008-icra-plicp.pdf">2008 ICRA-PLICP</a> by Andrea Censi.</p>

<h2 id="f1tenth-lab5-assignment">F1Tenth Lab5 assignment</h2>
<p>My final package as required in the task is available bellow.</p>

<p>TODO</p>

<p>pdf with theoretical part and txt with video links of the assignment included in the zipfile.</p>

<p>TODO VIDEO</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Deeper dive into localization with scan matching.]]></summary></entry><entry><title type="html">Simultaneous Localization and Mapping - SLAM</title><link href="http://localhost:4000/2020/11/13/slam.html" rel="alternate" type="text/html" title="Simultaneous Localization and Mapping - SLAM" /><published>2020-11-13T00:00:00+01:00</published><updated>2020-11-13T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/13/slam</id><content type="html" xml:base="http://localhost:4000/2020/11/13/slam.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Moving away from reactive methods to different approach to autonomy and that’s utilising mapping information around the car. This method is called Simultaneous Localization and Mapping or for short SLAM. I’m going to describe some building blocks for SLAM such as Occupancy grid maps and also describe some of the basic SLAM algorithms out there and how they work. Localization is problem if we are given the map of environment we should figure out where we are located in the map. Mapping problem is complimentary to localization, knowing the pose of the car we need to build map of environment using the sensors available to us. SLAM is combination of these two to use these data to estimate trajectory of the robot and build the map at the same time. SLAM is way more beneficial over algorithms such as wallfollowing or follow the map because it allow us to plan longterm path of the robot and not only reacting to the momentary information.</p>

<h2 id="occupancy-grid-map">Occupancy Grid Map</h2>
<p>Occupancy can be though as binary random variable which can take 2 values: Free, Occupied.</p>

<p>Formal definition is:
m<sub>x,y</sub>:{free, occupied} -&gt; {0, 1}</p>

<p>Grid map is sequence of cell where each cell has occupancy variable, either free or occupied.</p>

<h4 id="occupancy-grid-mapping">Occupancy grid mapping:</h4>
<p>We want to assign a probability to every cell of the grid if the cell is free or occupied based on the robot’s data. To do this we use Bayesian filtering. Where we look at prior and recursively update the status of probability for each cell. To do so we need to use Bayes rule. We also need information from the measurements of the robot to tell us where is the obstacle located in the frame of reference of the robot. So the measurements for each cell can be reported either free or occupied, so we can think of this as a likelihood of observing certain measurement given what we know about the cells. To describe this formally:</p>

<p>Measurement model:
p(z|m<sub>x,y</sub>) where z~{0,1}
probability of z(measurement) conditioned upon what we know about the value of m(occupancy random variable) of the occupancy grid.</p>

<p>we can have these different measurement models only:
p(z = 1|m<sub>x,y</sub> = 1)-&gt; TRUE occupied measurement,
p(z = 0|m<sub>x,y</sub> = 1)-&gt; FALSE free measurement,
p(z = 1|m<sub>x,y</sub> = 0)  -&gt; FALSE occupied measurement,
p(z = 0|m<sub>x,y</sub> = 0)  -&gt; TRUE free measurement.</p>

<p>So taking all of these together into bayes rule:
<img src="/assets/bayes.png" alt="bayes" />
Given a occupancy grid (Prior map), we measure the data from the robot and we have a probability associated with observing 0 or 1 conditioned on state itself (measurement model) and we use this to update our knowledge of the occupancy of the grid cell or computing the posterior map ( the probability of cell being occupied of free conditioned upon what the measurement has reported ). This can be done by applying Bayes rule as shown in figure above (ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>).</p>

<p>But working with these probabilities can become mathematically inconvenient. Instead it is better to work with Odds.
<img src="/assets/odd1.png" alt="odd1" />
So given the formula above ( ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>) we can see that odd is probability of X happening divided by probability of X not happening so basically ratio of probabilities.
More specifically the Odd that a certain cell is occupied given z is a probability that the cell is occupied given z divided by probability that cell is not occupied given z as shown in the formula bellow ( ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>).
<img src="/assets/odd2.png" alt="odd2" />
By applying Bayes rule to the numerator:
<img src="/assets/bayes_odd.png" alt="bayes_odd" />
As well as by applying Bayes rule to the denominator:
<img src="/assets/bayes_odd2.png" alt="bayes_odd2" />
The evidence term p(z) can be eliminated. And on the resulting equation the log can be applied resulting in log in sum of two odds which results in the new log odd being equal to the previous log odd + log odd of the measurement which is easier to compute and which denotes the same thing as non-log version. In formulas shown below ( ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>):</p>

<p><img src="/assets/odd3.png" alt="odd3" /></p>

<p>So the measurement model in the log odd version will become:
<img src="/assets/measurementodd.png" alt="measurementodd" /></p>

<p>Do not forget that every grid cell is updated at each point in time. Also we haven’t explore where is the robot in the map. As well as the description above is considering only one single lidar beam scan. Let’s explore this further. If you haven’t check the previous post on ROS transformations, because it is needed to understand the next section.</p>

<h3 id="range-measurement-on-the-grid">Range measurement on the Grid</h3>
<p>Having map frame and base_link frame, we assume that we get measurement along primary x1 direction d distance from the abse link of the robot.At any given time we know the pose of the robot. We can use this information to find out that the reported occupied cell is d distance away from the base link of the robot and we can figure out coordinates of this point and express it in the global frame of reference using rigid body transformation equation shown bellow described in the previous post.
<img src="/assets/tf.png" alt="tf" />
All this was done in continuous space. To convert it to grid space we have to know the resolution of the grid r and than we can  simply put the point in grid i where i = ceil(x/r), of course because we have x<sub>1</sub> and x<sub>2</sub> we have to calculate both i<sub>1</sub> and i<sub>2</sub>. Of course the robot is giving us a lot of measurements at any given time, not only one. In this case we have to also account for α<sub>k</sub>, which stands for single lidar beam of the robot. All of what I described is much better shown on the image bellow ( ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>).
<img src="/assets/tf2.png" alt="tf2" /></p>

<h2 id="google-cartographer">Google cartographer</h2>
<p>This algorithm is very effective in the part of SLAM called loop closure, it reduce computational requirements of loop closure.</p>

<h3 id="loop-closure">Loop closure</h3>
<p>We know that there are errors accumulating over time as the robot is updating the grid map measurement so it can happen that the map can start shifting from the actual word. If I revisit a place on map which I have visited before we can correct all this accumulated error. Basically we can overlap regions of map which are found to be the same based on this point of map where we know we have already been. Here is very nice video of showing this in action:</p>
<iframe width="800" height="443" src="https://www.youtube.com/embed/-EQAJOoRqEQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Moreover google cartographer is divided into two sub-system.</p>

<h4 id="local-slam">Local SLAM</h4>
<p>The job of local SLAM is generate good submaps. Submap is defined by how much data the robot received within a threshold. Submap must be small enough so the uncertainty of localization  is bellow the resolution of occupancy grid, so they are locally correct. However at the same time they have to be large enough to be different enough for the loop closure work correctly.</p>

<h4 id="global-slam">Global SLAM</h4>
<p>The job of local SLAM is to tie submaps consistently together as well as loop closure.</p>

<p><a href="https://google-cartographer.readthedocs.io/en/latest/">The Google cartogpraher documentation</a> is great resource to find out more details about this very important and used algorithm in ROS and whole robotic.</p>

<h2 id="summary">Summary</h2>
<p>SLAM is probably one of the most important concept in robotics, so it is definitely worth of post by itself. I described what is occupancy grid map, how to update it cells by Bayesian filtering, log odds to update the probability of cell being free or occupied and briefly looked at Google cartographer. There is no assignment in the post as there is more I need to study before I start on the assignment on scan matching in lab5.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Looking into algorithm which should solve the chicken-and-egg problem of an autonomous agent localising itself in an unknown environment while mapping this environment at the same time.]]></summary></entry><entry><title type="html">Follow the Gap</title><link href="http://localhost:4000/2020/11/10/follow_the_gap.html" rel="alternate" type="text/html" title="Follow the Gap" /><published>2020-11-10T00:00:00+01:00</published><updated>2020-11-10T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/10/follow_the_gap</id><content type="html" xml:base="http://localhost:4000/2020/11/10/follow_the_gap.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Let’s describe alternative approach to wallfollowing, which is just following left or right hand-side boundary of track. The Follow the Gap algorithm instead tracing the boundary is navigating trough the track without colliding to any static obstacle and finding the best path to avoid these obstacles. The wallfollowing is not completely able to solve all challenges coming with having the obstacles in the track, which are different shape. Follow the gap is simple algorithm which doesn’t need a lot fo information about the track before-hand, but using the data acquired right now, known as reactive method. It is able to avoid any obstacles without any prior knowledge about the map.</p>

<h2 id="follow-the-gap">Follow the Gap</h2>
<h4 id="reactive-navigation">Reactive navigation</h4>
<p>Reactive navigation is using immediate sensory input to decide the driving steering and velocity commands. This works for statics and in some cases also dynamic obstacles.</p>

<h3 id="gap-finding-intuition">Gap finding intuition</h3>
<p>As the name of algorithm suggests, the car should be able to find a widest gap in the presence of immediate obstacles and drive into this gap to avoid any immediate collision. Let’s say my lidar is reporting following array of distances [0.2, 6.2, 6.0, 7.0, inf, 3.0, inf, 3.0, inf, 8.0, 1.0, 3.0] you could simply choose the farthest distance obstacle and drive towards it, however this may not be possible as the car would have to pass between too obstacles which range between them is not enough for car to fit in ( there is enough room in between than - this can be computed by computing length of the arc given the angular measurements), or the angle between these obstacles is not physically possible for car to turn - as in my array is case of 7th element. To approach this problem let’s define what a gap is:</p>
<ul>
  <li>The gap is series of at least n consecutive hits that pass some distance threshold t.</li>
</ul>

<p>If we would apply this definition to the array given above with n = 3 and t = 5.0 we would get only one gap: [6.2, 6.0, 7.0, inf]. So we would steer towards the center of this gap.</p>

<p>Problems with this approach is that purely following the deepest gap allows the car pass between the obstacles as the car might not fit in between them. To address this challenge we can use approach use in all of robotics.</p>

<h4 id="point-robot-approach">Point Robot Approach</h4>
<p>We can assume that the robot is circular. With this assumption we can inflate the  the obstacles with the radius of robot. So if I want the robot go between the obstacles I can always represent the robot as point object and still assure that the robot will fit between the two obstacles as I’m clearing the obstacles by at least the radius of robot itself. This is better dhown on figure bellow (source: <a href="https://linklab-uva.github.io/autonomousracing/assets/files/L14-Follow-the-gap.pdf">UV lecture follow the gap</a>):
<img src="/assets/PRA.png" alt="PRA" /></p>

<h4 id="disparity">Disparity</h4>
<p>Looking trough the lidar readings for consecutive readings that differ by an amount over some threshold, than we mask these readings  to appear closer to the lidar. The reminding points which are farther than the mask are the actual points where the car can fit and drive trough. This concept is nicely visualised on the following figure from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/L14-Follow-the-gap.pdf">UV lecture follow the gap</a>
<img src="/assets/ftg1.png" height="500" /></p>

<h3 id="follow-the-gap-algorithm">Follow the gap algorithm</h3>
<ul>
  <li>step 1:
Find the nearest LIDAR point and pit safety bubble around it of radius rb</li>
  <li>step 2:
Set all points inside bubble to distance 0. This can be achieved by computing the length of the arc and than determine which points falls into the radius of buble rb. All of these points are than set to 0.</li>
  <li>step 3:
Find maximum sequence of consecutive non-zeros among the free-space points. This is the maximum gap where the car can drive.</li>
  <li>step 4:
Find the best point among this maximum sequence from previous step.</li>
</ul>

<p>To make this as fast as possible, I should be looking from farther ranges for these obstacles, because doing sharp turns results in slower velocity, which can be achieved by turning earlier in less sharp angle.</p>

<h4 id="disadvantages">Disadvantages</h4>
<p>There is risk of doing U turns if he car is going too fast and it has to do 90 degrees turn which it can’t currently see as at particular point it will see the point on the opposite side of track, rather than the turn itself. This is shown on the figure bellow for better intuition.
<img src="/assets/ftg_fail.png" height="500" /></p>

<p>It is also shown in the nice follow the gap practical demonstration in video bellow.</p>
<iframe width="700" height="400" src="https://www.youtube.com/embed/ctTJHueaTcY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="f1tenth-lab4-assignment">F1Tenth Lab4 assignment</h2>
<p>My implementation of FtG algorithm consists of these steps:</p>
<ol>
  <li>Process each LiDAR scan by considering only field of view of 90 (45 to left and 45 to right as the 0th degree is exactly infront of the car) degrees, setting each value to the running mean of window 7 by convolution and clipping all values over 3m to 3m</li>
  <li>Find the closest obstacle and set all points in bubble around it in certain radius 0. This bubble is computed by computing arc of proportion of radius and closest point and computing all angles inside this bubble. Ranges corresponding to the indexes of angles which are in the bubble are than set to 0.</li>
  <li>Finding the max gap by finding the longest non-zero consecutive values in the indexes of the processed lidar scan</li>
  <li>Finding the deepest possible scan and steer in the direction of it.</li>
</ol>

<iframe width="800" height="400" src="https://www.youtube.com/embed/pKxiRvM4X6U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>]]></content><author><name></name></author><summary type="html"><![CDATA[Addressing the shortcomings of wallfollowing and implementing Follow the Gap algorithm.]]></summary></entry><entry><title type="html">Wallfollowing</title><link href="http://localhost:4000/2020/11/06/wallfollowing.html" rel="alternate" type="text/html" title="Wallfollowing" /><published>2020-11-06T00:00:00+01:00</published><updated>2020-11-06T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/06/wallfollowing</id><content type="html" xml:base="http://localhost:4000/2020/11/06/wallfollowing.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Lecture 3 as prerequisite for wallfollowing algorithm covers one of the last ROS introductory topic as that are Rigid Body Transformation.
Coordinate frames in ros such as map frame, lidar frame and other sensors provide informations specific to that sensor. Coordinate frame is
set of 3 orthogonal axes for X,Y and Z direction with a position where this frame is placed. Any position of word, robot, etc. only makes sense
when we defined the frame in which we are describing the pose. The direction of X,Y, Z axes is defined by right-handed rule, where index finger points in
X direction, middle finger in y direction and thumb in Z direction. Let’s look more into transformations and frames.</p>

<h2 id="transfomations-and-frames">Transfomations and Frames</h2>

<p>Why do we need transformations between different sensors? Each time I get data in sensor frame (for specific sensor) which I want to transform in some unified way.
For example we can have frame of reference of lidar which we can transform into the frame of reference of robot, which is the center of the rear axle. As example in
the last assignment we only take in account the frame of reference of lidar and stopped the car based on position of the lidar. However car have certain length and lidar
can be place in the center of the car. Therefore the correct way of stopping before collision should be calculated from the front of the car (or edge of the car), not from the
position of the lidar. Between frames there will exist transformation that convert measuring from one frame to another.</p>

<h2 id="reference-frames-on-f1tenth-car">Reference Frames on F1Tenth car</h2>

<h4 id="map">map</h4>
<p>The origin set by the user and every other transformation can be measured with respect to the map. Represent the environment where the car will be racing. Can be place arbitrary and typically never moves after its placed.</p>

<h4 id="base_link">base_link</h4>
<p>Positioned at center of the rear axle of the car. Moves with the car relative to the map frame.</p>

<h4 id="lidar">lidar</h4>
<p>The frame of reference where lidar scan measurements are taken. Moves with the car relative to the map frame.</p>

<h4 id="odom">odom</h4>
<p>Typically required by ROS. Usually the initial position of the robot in the map before everything began. Fixed relative to the map.</p>

<h3 id="rigid-body-transfomation">Rigid Body Transfomation</h3>

<p>How to transform data and position between one frame and another. As on figure bellow, we want the coordinate of point p in frame 2, given the coordinate of point p in frame 1.</p>

<p><img src="/assets/frames.png" alt="frames" />
2 diffrent coordinate frames and point p from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a>
  Steps:</p>
<ul>
  <li>Overlap both frames so their origins are at the same point. The reason is that we want to compute how is second frame of reference rotated. (apply rotation)</li>
  <li>Describe the unit vectors of the second frame of reference in the terms of unit vectors of first frame of reference. As on picture bellow.</li>
</ul>

<p><img src="/assets/frames2.png" alt="frames2" />
Overlapped frames of reference from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a></p>

<p>x<sub>2</sub> = R<sub>11</sub>x<sub>1</sub> + R<sub>21</sub> y<sub>1</sub><br />
y<sub>2</sub> = R<sub>12</sub>x<sub>1</sub> + R<sub>22</sub> y<sub>1</sub><br />
The formula above describes the units vector of new frame of referece as a linear combination of the original frame of reference. (considering only X,Y axis - 2D problem, no Z).
The formula can be also rewritten as matrix:</p>

<p><img src="/assets/rotation_matrix.png" alt="rotation_m1" />
Rotation matrix from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a></p>

<p>Also we define θ as angle between x<sub>1</sub> and x<sub>2</sub> as shown on figure bellow. This angle tells us how rotated this frame is.</p>

<p><img src="/assets/theta.png" alt="theta" />
Overlapped frames of reference with angle theta from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a></p>

<p>To get the coefficients R<sub>11</sub>, R<sub>21</sub>, R<sub>12</sub>, R<sub>22</sub>, along x<sub>2</sub> unit vector direction, contains the cos unit component of x<sub>1</sub>
unit vector as well as sin component of y<sub>1</sub> unit vector. Thefore  x<sub>2</sub> and similary y<sub>2</sub> can be written as:</p>

<p>x<sub>2</sub> = cos(θ)x<sub>1</sub> + sin(θ)y<sub>1</sub><br />
y<sub>2</sub> = -sin(θ)x<sub>1</sub> + cos(θ)y<sub>1</sub><br /></p>

<p>Therefore I get:</p>

<p><img src="/assets/rotation_matrix2.png" alt="rotation_m" />
Rotation matrix from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a></p>

<p>With this we are able to represent unit vectors in the new frame of reference in terms of unit vectors of original frame of reference. Similarly we can express the position of point P in the new frame of reference using the rotation matrix as shown on formulas bellow.</p>

<p><img src="/assets/rotation_p.png" alt="rotation" />
Rotation matrix of point p from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a>.
We can’t forget to apply the translation as well.</p>

<h4 id="ros-tftf2-package">ROS tf/tf2 package</h4>
<p><a href="http://wiki.ros.org/tf2">ROS tf2 package</a> lets you keep track of multiple coordinate frames over time. And transform points/poses between two coordinates. It broadcast this across ROS so any node can subscribe to it. To get more insight how it works it is very useful to try <a href="http://wiki.ros.org/tf2/Tutorials">TF2 Tutorial</a>, older <a href="http://wiki.ros.org/tf/Tutorials">TF Tutorial</a> and <a href="http://wiki.ros.org/tf2/Migration">TF2 Migration</a>.</p>

<h4 id="wallfollowing">Wallfollowing</h4>
<p>The idea is we are trying to compute the error between the future position of the car instead of current error because of constant movement of the car.
By minimising the future distance from the wall with respect to the optimal trajectory we are able to follow the wall.
Lets visualise the algorithm equations with the help from figures bellow. All of them taken from the <a href="https://linklab-uva.github.io/autonomousracing/assets/files/Wall_Following.pdf">University of Virginia F1/10 Course from
Madhur Behl</a>.
<img src="/assets/wallfollowing1.png" alt="wallfollowing1" /></p>

<p><img src="/assets/wallfollowing2.png" alt="wallfollowing2" /></p>

<p><img src="/assets/wallfollowing3.png" alt="wallfollowing3" /></p>

<p>If at any given point my car is at point A, the distance to the wall is B, but as already mentioned, I’m not trying to compute the error between B and the desired trajectory of the car visualised on the last figure as vertical green lines. You must project the car forward based on the velocity of the car and minimise the future distance to the wall with respect to the desired trajectory (CD in the case of figure). Using trigonometry the angle α can be computed knowing distances a and b which are the projected distances to the wall by lidar and corresponding angle between a and b called θ. Knowing the angle α I’m able to compute the current distance to the wall as well as future distance CD.</p>

<h4 id="pid">PID</h4>

<p>Using CD I can compute the error which is just difference between desired trajectory and CD. This error is important because we need it to setup the proportional and derivative controller to correct the steering angle of the car in the PID manner with respect to error.
 Formula for PID in the figure bellow (again from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/Wall_Following.pdf">UV wallfollowing lecture</a>)</p>

<p><img src="/assets/pid.png" alt="pid" /></p>

<p>V<sub>θ</sub> being correction in steering, K<sub>p</sub> is proportion of my error  computed above as desired trajectory - CD. K<sub>d</sub> is derivative gain and the rate of change of error <sup>de(t)</sup>⁄<sub>dt</sub>. This can be simplified by computing previous error - current error. Once I have the error correction in steering, I can update the steering angle angle with the correction. If the PID constants are well tuned, the steering angle should correct itself based on how far the car is from desired trajectory.</p>

<h2 id="f1tenth-lab3-assignment">F1Tenth Lab3 assignment</h2>
<p>Moving on to my implementation of the WallFollow node. The algorithm implemented consists of these steps:</p>
<ul>
  <li>Step 1. Obtain two laser scans (distances) a and b, with b taken at 0 degrees and a at an angle theta (0 &lt; theta =&lt; 70),
    <ul>
      <li>the 0th angle corresponds to the front of the f1tenth car and the positive angle direction corresponds to the left of the car, therefore choice of 0 degree angle and in my case 60 degree angle corresponds to the left wall following.</li>
    </ul>
  </li>
  <li>Step 2. Use the distances a and b to calculate the angle alpha between the car’s x- axis and the left wall and use alpha to find the current distance D_t to the car,
    <ul>
      <li>all of these are calculated using the formulas described in the wallfollowing section.</li>
    </ul>
  </li>
  <li>Step 3. And than alpha and D_t to find the estimated future distance D_t1 to the wall</li>
  <li>Step 4. Run D_t1 trough the PID algorithm described above ( in assignment) to get a steering angle</li>
</ul>

<p>I was experimenting for a long time with different values of K<sub>p</sub>,K<sub>d</sub> and K<sub>i</sub>. Using this I was able to find get a more insight what each of these constant is doing. I’m not 100% sure about, but I think that lowering K<sub>p</sub> makes the car less responsive to the error magnitude, as increasing this constant makes car move move with a twisting motion, K<sub>d</sub> seems to react faster to the growing error but it wasn’t completely clear to me from simulation. K<sub>i</sub> didn’t seem to do much even at higher values, but according to the research it should make the car more sensitive to error.</p>

<p>I’ve found that setting the right theta had probably biggest impact on the car ability to follow the wall without bouncing to the corners. Also one additional thing while filtering nan and inf values from the scan reading, I also removed the readings that are higher angle that the lidar sensor on real car is able to read, which is apparently 270 degrees. The car in the simulation has no problem reading in 360 dergees so there are none infinite or nan values in simulation.</p>

<iframe width="800" height="400" src="https://www.youtube.com/embed/5nLtlszkRvI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>]]></content><author><name></name></author><summary type="html"><![CDATA[Fist autonomous lap of my F1Tenth car using wallfollowing algorithm.]]></summary></entry></feed>