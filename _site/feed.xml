<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-11-23T18:21:41+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">My F1Tenth journey</title><subtitle>My learning process and notes, including assignments from the F1Tenth 1/10 formula autonomous racing.</subtitle><entry><title type="html">Pure Pursuit</title><link href="http://localhost:4000/2020/11/19/pure-pursuit.html" rel="alternate" type="text/html" title="Pure Pursuit" /><published>2020-11-19T00:00:00+01:00</published><updated>2020-11-19T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/19/pure-pursuit</id><content type="html" xml:base="http://localhost:4000/2020/11/19/pure-pursuit.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Very effective algorithm all the way from 1992 is based on very simple idea of following a waypoint. It is tracking algorithm with assumption that we already know where the set of waypoints is known both in global and local coordinate. This can be done by cartographer SLAM algorithm for example.&lt;/p&gt;

&lt;h3 id=&quot;geometric-representation&quot;&gt;Geometric representation&lt;/h3&gt;
&lt;p&gt;Starting from car’s local frame (base_link), we need to define so called Lookahead distance L, which is simply radius around the base_link where we are looking for waypoint. Because F1Tenth is nonholonomic robot, to get to the waypoint we need to turn along some arc, it is not possible to move to the waypoint in straight direction. However there can be indefinitely many arcs define to a single point as sown on the figure bellow (ref &lt;a href=&quot;https://f1tenth.org/learn.html&quot;&gt;f1tenth Module D, lecture 13&lt;/a&gt;).
&lt;img src=&quot;/assets/pure_pursuit_geometric.png&quot; alt=&quot;pp&quot; /&gt;
To solve this, we need to constrain the centre of the arc to be on y axes of the car. Using this we can start solving the equations bellow using basic trigonometry. Because we constrained the centre on y axis as radius of the arc, r can be computed as r = |y| + d. We also have right angle triangle therefore d&lt;sup&gt;2&lt;/sup&gt; + x&lt;sup&gt;2&lt;/sup&gt; = r&lt;sup&gt;2&lt;/sup&gt;. Substituting d as the first equation r = |y| + d we get (r - |y|)&lt;sup&gt;2&lt;/sup&gt; + x&lt;sup&gt;2&lt;/sup&gt; = r&lt;sup&gt;2&lt;/sup&gt; and solving this equation we get r&lt;sup&gt;2&lt;/sup&gt; + L&lt;sup&gt;2&lt;/sup&gt; - 2r|y| = r&lt;sup&gt;2&lt;/sup&gt;. The L is again from simple Pythagorean theorem because we know x&lt;sup&gt;2&lt;/sup&gt; and y&lt;sup&gt;2&lt;/sup&gt; in the triangle we can express L as L&lt;sup&gt;2&lt;/sup&gt; = x&lt;sup&gt;2&lt;/sup&gt; + y&lt;sup&gt;2&lt;/sup&gt; shown on the picture bellow (ref &lt;a href=&quot;https://f1tenth.org/learn.html&quot;&gt;f1tenth Module D, lecture 13&lt;/a&gt;)
&lt;img src=&quot;/assets/pp_geometric2.png&quot; alt=&quot;pp2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Solving r&lt;sup&gt;2&lt;/sup&gt; + L&lt;sup&gt;2&lt;/sup&gt; - 2ry = r&lt;sup&gt;2&lt;/sup&gt; for r we have r = L&lt;sup&gt;2&lt;/sup&gt;/2y. Since we know the radius, we can compute the curvature of arc as inverse of radius γ = 1/r = 2y/L&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;picking-a-goal-point&quot;&gt;Picking a goal point&lt;/h3&gt;
&lt;p&gt;There is many choices of how to pick a waypoint in the lookahead radius L, the simplest one is probably either choice the farthest waypoint the the L circle or closest outside of circle. Another example could be get both the closest point outside of circle and farthest point inside of circle, connect them and set a waypoint to the intersection of this connection and circle of lookahead distance. Visualised on figure bellow (ref &lt;a href=&quot;https://f1tenth.org/learn.html&quot;&gt;f1tenth Module D, lecture 13&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pp_waypoint.png&quot; alt=&quot;waypoint&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;tuning&quot;&gt;Tuning&lt;/h3&gt;
&lt;p&gt;The main thing we need to is the lookahead distance L. Smaller L acts same as higher Kp in PID control where the car is gonna be more aggressive and do higher curvatures and oscillations, with higher L is the opposite, the trajectory will be smoother, but the tracking error will be higher.&lt;/p&gt;

&lt;h3 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h3&gt;
&lt;p&gt;(ref &lt;a href=&quot;https://f1tenth.org/learn.html&quot;&gt;f1tenth Module D, lecture 13&lt;/a&gt;)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Create a map using Cartographer&lt;/li&gt;
  &lt;li&gt;Create a set of waypoints using global planner (e.g. by driving by teleop)&lt;/li&gt;
  &lt;li&gt;Pick waypoints to track at each frame&lt;/li&gt;
  &lt;li&gt;Set steering angle to track current waypoint&lt;/li&gt;
  &lt;li&gt;Update the waypoint on the go&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;f1tenth-lab6-assignment&quot;&gt;F1Tenth Lab6 assignment&lt;/h2&gt;

&lt;h3 id=&quot;encountered-difficulties&quot;&gt;Encountered difficulties&lt;/h3&gt;
&lt;p&gt;During this assignment for the first time I experienced some longer sessions on front of monitor to resolve some of the issues I had, as well as had problems with my programmings skills themselves.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Cartographer
I was able to make cartographer work locally as shown on examples on &lt;a href=&quot;https://google-cartographer-ros.readthedocs.io/en/latest/demos.html&quot;&gt;https://google-cartographer-ros.readthedocs.io/en/latest/demos.html&lt;/a&gt;, after that I moved the files as mentioned in the &lt;a href=&quot;https://youtu.be/L51S2RVu-zc?t=4403&quot;&gt;pure_pursuit lecture video&lt;/a&gt;, I had to change in file F110.xarco line 194 from
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &amp;lt;mesh &lt;span class=&quot;nv&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;package://racecar_description/meshes/hokuyo.dae&quot;&lt;/span&gt;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;to&lt;/p&gt;
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &amp;lt;mesh &lt;span class=&quot;nv&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;package://f110_description/meshes/hokuyo.dae&quot;&lt;/span&gt;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;From this I suppose that this should be only related to the real car, as I’m not able to make the model of the car work in the simulation of RVIZ, where RVIZ have problem transforming some parameters related to the car. So instead of running cartographer in unknown map I used one of the map that was saved in f1tenth_simulator package already and run waylogger in this map.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Switching from Python 3 to Python2.7
In order to run waylogger I needed to install package with Particle Filter. For this I used &lt;a href=&quot;https://github.com/mit-racecar/particle_filter&quot;&gt;MIT RACE CAR ROS PACKAGE&lt;/a&gt; as I didn’t find this package anywhere in the f110. Up until this point I was using python3 without any problems in the assignments so far however this particle_filter and waylogger package both were using python2.7 so I decided to switch my ROS melodic to python2. However setting .bashrc file to python 2.7 didn’t switch my ROS system to python2 and was still using python3. I spent long hours trying to figure this out because I think as I was using anaconda with python3 there were some settings which prevented the any system in my ubuntu from using python2 and even completely reinstalling whole ROS melodic didn’t and removing anaconda reference from .bashrc file didn’t work. I had to uninstall the anaconda completely as well as uninstall and install back ROS.&lt;/li&gt;
  &lt;li&gt;Waypoint Logger
After reinstalling my whole ROS the waylogger was finally working, however it needed running particle_filter topic otherwise it didn’t log anything. In the above mentioned particle filter for MIT car I had to change &lt;a href=&quot;https://github.com/mit-racecar/particle_filter/blob/master/launch/localize.launch&quot;&gt;localize.launch&lt;/a&gt;, concretely comment line 30
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; 	&amp;lt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; &amp;lt;include &lt;span class=&quot;nv&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;find particle_filter&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/launch/map_server.launch&quot;&lt;/span&gt;/&amp;gt; &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;as I was using my own map server for the map from f1tenth_simulator package
and in line 33 use only /odom topic&lt;/p&gt;
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; 	&amp;lt;arg &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;odometry_topic&quot;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/odom&quot;&lt;/span&gt;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Launching waylogger and particle_filter
Strangely enough to get correct coordinates in logfile the particle_filter node needs to be launched as first, before the map and rviz are launched, otherwise the resulting coordinate system is different than the map launched.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After all of this above I was able to generate proper logfile with right coordinates.&lt;/p&gt;

&lt;p&gt;Probably most difficulties during the programming of assignment was to correctly transform the global and car correctly otherwise the rest was very straight forward using the formulas from lecture. I expected to have more problems with the visualization, however the ROS Marker tutorial is easy to follow and the visualization was also quickly implement.&lt;/p&gt;

&lt;p&gt;My final package as required in the task is available bellow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/smejkka3/smejkka3.github.io/raw/master/assets/karel_lab6.zip&quot;&gt;karel_lab6.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;txt with video link of the assignment included in the zipfile.&lt;/p&gt;
&lt;iframe width=&quot;800&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/sAk8qmBD_LI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;iframe width=&quot;800&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/aFKKos5si0Y&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name></name></author><summary type="html">First practical experience with Cartographer SLAM package and implementation of trajectory planner called the Pure Pursuit algorithm.</summary></entry><entry><title type="html">Scan matching</title><link href="http://localhost:4000/2020/11/14/scan_matching.html" rel="alternate" type="text/html" title="Scan matching" /><published>2020-11-14T00:00:00+01:00</published><updated>2020-11-14T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/14/scan_matching</id><content type="html" xml:base="http://localhost:4000/2020/11/14/scan_matching.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Last post was about SLAM, where I introduced approach to localization, here we are going to look at approach to lacalization using sets of range measurements collected over across time. Going back, localization is state of robot with respect to the environment it find self in. One approach to localization is to use odometry, basically estimating the current pose of the robot from knowing a start position and integration of control and motion measurements. However there is accumulation of very small errors in the position of the robot which becomes issue over some extended period time and the uncertainty of the pose increases. One way to solve this is to use range sensor measurements to localize, also known as scan matching.&lt;/p&gt;

&lt;h2 id=&quot;setting-a-problem&quot;&gt;Setting a Problem&lt;/h2&gt;
&lt;p&gt;Having a scan from robot of measurements of distances to some landmarks A,B,C in some environment, we call this measurements at t=0 distances in local frame of reference. When we move in unknown direction at time t=1, the distances are obviously going to change. We can measure these distances at time t=1 and we want find transform R which transforms the two set of points to be the closest and this transform represents how much the robot moved. This is visualised on the picture bellow for better intuition ( ref &lt;a href=&quot;https://f1tenth.org/learn.html&quot;&gt;module C, lecture 9 F1Tenth website&lt;/a&gt;)
&lt;img src=&quot;/assets/problem_scan_matching.png&quot; alt=&quot;problem_scan_matching&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However only from scan readings alone, we can’t know the landmarks A,B,C exactly, therefore we have to assume that the closest points correspond to each other(correspondence match) and than iteratively find the best transform R.&lt;/p&gt;

&lt;h3 id=&quot;iterative-search-for-the-best-transform&quot;&gt;Iterative search for the best transform&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Make a initial guess of R&lt;/li&gt;
  &lt;li&gt;For each point in new scan (t=k+1) find closest point in previous set (t=k) -&amp;gt; correspondence search&lt;/li&gt;
  &lt;li&gt;Make another better guess of R&lt;/li&gt;
  &lt;li&gt;Set next guess to the current guess (repeat 2-4 until converges)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For this algorithm (Iterative closest point or ICP) to converge, the choice of initial guess is important. Also ICP can be slow. For this reason PL-ICP was introduced (Point-to-Line Iterative Closest Point).&lt;/p&gt;

&lt;h3 id=&quot;point-to-line-iterative-closest-point&quot;&gt;Point-to-Line Iterative Closest Point&lt;/h3&gt;
&lt;p&gt;Instead of looking for point to point metric, PL-ICP is looking for point-to-line metric. Point-to-point measures the distances the distance to the nearest point on the segment, while point-to-line measures distance to nearest line containing the segment. Point-to-Line converges faster because the level sets the approximate surface better and therefore results in more accurate approximation or the error. This is achieved by the contours of the level being line and not concentric and centred around the projected point and thus giving algorithm quadratic convergence instead of linear convergence.&lt;/p&gt;

&lt;h4 id=&quot;pl-icl-algorithm&quot;&gt;PL-ICL algorithm&lt;/h4&gt;
&lt;p&gt;at t=k&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;use previous guess q_k, transform coordinates of current scan into frame of previous scan&lt;/li&gt;
  &lt;li&gt;For each point, find closest line segment (correspondence search)&lt;/li&gt;
  &lt;li&gt;Update transform
  a. formulate point-to-line-error objective
  b. find transform q&lt;sub&gt;k+1&lt;/sub&gt; that minimizes objective&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;PL-ICL also uses smart “tricks” in correspondence match search. It does local search with early termination and jump table for each scan point.&lt;/p&gt;

&lt;p&gt;TODO DO MORE RESEARCH IN PL-ICL AND DESCRIBE BETTER POSSIBLY AFTER ASSIGNMENT.&lt;/p&gt;

&lt;p&gt;To understand this algorithm better I recommend read trough original paper &lt;a href=&quot;https://censi.science/pub/research/2008-icra-plicp.pdf&quot;&gt;2008 ICRA-PLICP&lt;/a&gt; by Andrea Censi.&lt;/p&gt;

&lt;h2 id=&quot;f1tenth-lab5-assignment&quot;&gt;F1Tenth Lab5 assignment&lt;/h2&gt;
&lt;p&gt;My final package as required in the task is available bellow.&lt;/p&gt;

&lt;p&gt;TODO&lt;/p&gt;

&lt;p&gt;pdf with theoretical part and txt with video links of the assignment included in the zipfile.&lt;/p&gt;

&lt;p&gt;TODO VIDEO&lt;/p&gt;</content><author><name></name></author><summary type="html">Deeper dive into localization with scan matching.</summary></entry><entry><title type="html">Simultaneous Localization and Mapping - SLAM</title><link href="http://localhost:4000/2020/11/13/slam.html" rel="alternate" type="text/html" title="Simultaneous Localization and Mapping - SLAM" /><published>2020-11-13T00:00:00+01:00</published><updated>2020-11-13T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/13/slam</id><content type="html" xml:base="http://localhost:4000/2020/11/13/slam.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Moving away from reactive methods to different approach to autonomy and that’s utilising mapping information around the car. This method is called Simultaneous Localization and Mapping or for short SLAM. I’m going to describe some building blocks for SLAM such as Occupancy grid maps and also describe some of the basic SLAM algorithms out there and how they work. Localization is problem if we are given the map of environment we should figure out where we are located in the map. Mapping problem is complimentary to localization, knowing the pose of the car we need to build map of environment using the sensors available to us. SLAM is combination of these two to use these data to estimate trajectory of the robot and build the map at the same time. SLAM is way more beneficial over algorithms such as wallfollowing or follow the map because it allow us to plan longterm path of the robot and not only reacting to the momentary information.&lt;/p&gt;

&lt;h2 id=&quot;occupancy-grid-map&quot;&gt;Occupancy Grid Map&lt;/h2&gt;
&lt;p&gt;Occupancy can be though as binary random variable which can take 2 values: Free, Occupied.&lt;/p&gt;

&lt;p&gt;Formal definition is:
m&lt;sub&gt;x,y&lt;/sub&gt;:{free, occupied} -&amp;gt; {0, 1}&lt;/p&gt;

&lt;p&gt;Grid map is sequence of cell where each cell has occupancy variable, either free or occupied.&lt;/p&gt;

&lt;h4 id=&quot;occupancy-grid-mapping&quot;&gt;Occupancy grid mapping:&lt;/h4&gt;
&lt;p&gt;We want to assign a probability to every cell of the grid if the cell is free or occupied based on the robot’s data. To do this we use Bayesian filtering. Where we look at prior and recursively update the status of probability for each cell. To do so we need to use Bayes rule. We also need information from the measurements of the robot to tell us where is the obstacle located in the frame of reference of the robot. So the measurements for each cell can be reported either free or occupied, so we can think of this as a likelihood of observing certain measurement given what we know about the cells. To describe this formally:&lt;/p&gt;

&lt;p&gt;Measurement model:
p(z|m&lt;sub&gt;x,y&lt;/sub&gt;) where z~{0,1}
probability of z(measurement) conditioned upon what we know about the value of m(occupancy random variable) of the occupancy grid.&lt;/p&gt;

&lt;p&gt;we can have these different measurement models only:
p(z = 1|m&lt;sub&gt;x,y&lt;/sub&gt; = 1)-&amp;gt; TRUE occupied measurement,
p(z = 0|m&lt;sub&gt;x,y&lt;/sub&gt; = 1)-&amp;gt; FALSE free measurement,
p(z = 1|m&lt;sub&gt;x,y&lt;/sub&gt; = 0)  -&amp;gt; FALSE occupied measurement,
p(z = 0|m&lt;sub&gt;x,y&lt;/sub&gt; = 0)  -&amp;gt; TRUE free measurement.&lt;/p&gt;

&lt;p&gt;So taking all of these together into bayes rule:
&lt;img src=&quot;/assets/bayes.png&quot; alt=&quot;bayes&quot; /&gt;
Given a occupancy grid (Prior map), we measure the data from the robot and we have a probability associated with observing 0 or 1 conditioned on state itself (measurement model) and we use this to update our knowledge of the occupancy of the grid cell or computing the posterior map ( the probability of cell being occupied of free conditioned upon what the measurement has reported ). This can be done by applying Bayes rule as shown in figure above (ref &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf&quot;&gt;UV SLAM lecture&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;But working with these probabilities can become mathematically inconvenient. Instead it is better to work with Odds.
&lt;img src=&quot;/assets/odd1.png&quot; alt=&quot;odd1&quot; /&gt;
So given the formula above ( ref &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf&quot;&gt;UV SLAM lecture&lt;/a&gt;) we can see that odd is probability of X happening divided by probability of X not happening so basically ratio of probabilities.
More specifically the Odd that a certain cell is occupied given z is a probability that the cell is occupied given z divided by probability that cell is not occupied given z as shown in the formula bellow ( ref &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf&quot;&gt;UV SLAM lecture&lt;/a&gt;).
&lt;img src=&quot;/assets/odd2.png&quot; alt=&quot;odd2&quot; /&gt;
By applying Bayes rule to the numerator:
&lt;img src=&quot;/assets/bayes_odd.png&quot; alt=&quot;bayes_odd&quot; /&gt;
As well as by applying Bayes rule to the denominator:
&lt;img src=&quot;/assets/bayes_odd2.png&quot; alt=&quot;bayes_odd2&quot; /&gt;
The evidence term p(z) can be eliminated. And on the resulting equation the log can be applied resulting in log in sum of two odds which results in the new log odd being equal to the previous log odd + log odd of the measurement which is easier to compute and which denotes the same thing as non-log version. In formulas shown below ( ref &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf&quot;&gt;UV SLAM lecture&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/odd3.png&quot; alt=&quot;odd3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So the measurement model in the log odd version will become:
&lt;img src=&quot;/assets/measurementodd.png&quot; alt=&quot;measurementodd&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Do not forget that every grid cell is updated at each point in time. Also we haven’t explore where is the robot in the map. As well as the description above is considering only one single lidar beam scan. Let’s explore this further. If you haven’t check the previous post on ROS transformations, because it is needed to understand the next section.&lt;/p&gt;

&lt;h3 id=&quot;range-measurement-on-the-grid&quot;&gt;Range measurement on the Grid&lt;/h3&gt;
&lt;p&gt;Having map frame and base_link frame, we assume that we get measurement along primary x1 direction d distance from the abse link of the robot.At any given time we know the pose of the robot. We can use this information to find out that the reported occupied cell is d distance away from the base link of the robot and we can figure out coordinates of this point and express it in the global frame of reference using rigid body transformation equation shown bellow described in the previous post.
&lt;img src=&quot;/assets/tf.png&quot; alt=&quot;tf&quot; /&gt;
All this was done in continuous space. To convert it to grid space we have to know the resolution of the grid r and than we can  simply put the point in grid i where i = ceil(x/r), of course because we have x&lt;sub&gt;1&lt;/sub&gt; and x&lt;sub&gt;2&lt;/sub&gt; we have to calculate both i&lt;sub&gt;1&lt;/sub&gt; and i&lt;sub&gt;2&lt;/sub&gt;. Of course the robot is giving us a lot of measurements at any given time, not only one. In this case we have to also account for α&lt;sub&gt;k&lt;/sub&gt;, which stands for single lidar beam of the robot. All of what I described is much better shown on the image bellow ( ref &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf&quot;&gt;UV SLAM lecture&lt;/a&gt;).
&lt;img src=&quot;/assets/tf2.png&quot; alt=&quot;tf2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;google-cartographer&quot;&gt;Google cartographer&lt;/h2&gt;
&lt;p&gt;This algorithm is very effective in the part of SLAM called loop closure, it reduce computational requirements of loop closure.&lt;/p&gt;

&lt;h3 id=&quot;loop-closure&quot;&gt;Loop closure&lt;/h3&gt;
&lt;p&gt;We know that there are errors accumulating over time as the robot is updating the grid map measurement so it can happen that the map can start shifting from the actual word. If I revisit a place on map which I have visited before we can correct all this accumulated error. Basically we can overlap regions of map which are found to be the same based on this point of map where we know we have already been. Here is very nice video of showing this in action:&lt;/p&gt;
&lt;iframe width=&quot;800&quot; height=&quot;443&quot; src=&quot;https://www.youtube.com/embed/-EQAJOoRqEQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Moreover google cartographer is divided into two sub-system.&lt;/p&gt;

&lt;h4 id=&quot;local-slam&quot;&gt;Local SLAM&lt;/h4&gt;
&lt;p&gt;The job of local SLAM is generate good submaps. Submap is defined by how much data the robot received within a threshold. Submap must be small enough so the uncertainty of localization  is bellow the resolution of occupancy grid, so they are locally correct. However at the same time they have to be large enough to be different enough for the loop closure work correctly.&lt;/p&gt;

&lt;h4 id=&quot;global-slam&quot;&gt;Global SLAM&lt;/h4&gt;
&lt;p&gt;The job of local SLAM is to tie submaps consistently together as well as loop closure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://google-cartographer.readthedocs.io/en/latest/&quot;&gt;The Google cartogpraher documentation&lt;/a&gt; is great resource to find out more details about this very important and used algorithm in ROS and whole robotic.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;SLAM is probably one of the most important concept in robotics, so it is definitely worth of post by itself. I described what is occupancy grid map, how to update it cells by Bayesian filtering, log odds to update the probability of cell being free or occupied and briefly looked at Google cartographer. There is no assignment in the post as there is more I need to study before I start on the assignment on scan matching in lab5.&lt;/p&gt;</content><author><name></name></author><summary type="html">Looking into algorithm which should solve the chicken-and-egg problem of an autonomous agent localising itself in an unknown environment while mapping this environment at the same time.</summary></entry><entry><title type="html">Follow the Gap</title><link href="http://localhost:4000/2020/11/10/follow_the_gap.html" rel="alternate" type="text/html" title="Follow the Gap" /><published>2020-11-10T00:00:00+01:00</published><updated>2020-11-10T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/10/follow_the_gap</id><content type="html" xml:base="http://localhost:4000/2020/11/10/follow_the_gap.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Let’s describe alternative approach to wallfollowing, which is just following left or right hand-side boundary of track. The Follow the Gap algorithm instead tracing the boundary is navigating trough the track without colliding to any static obstacle and finding the best path to avoid these obstacles. The wallfollowing is not completely able to solve all challenges coming with having the obstacles in the track, which are different shape. Follow the gap is simple algorithm which doesn’t need a lot fo information about the track before-hand, but using the data acquired right now, known as reactive method. It is able to avoid any obstacles without any prior knowledge about the map.&lt;/p&gt;

&lt;h2 id=&quot;follow-the-gap&quot;&gt;Follow the Gap&lt;/h2&gt;
&lt;h4 id=&quot;reactive-navigation&quot;&gt;Reactive navigation&lt;/h4&gt;
&lt;p&gt;Reactive navigation is using immediate sensory input to decide the driving steering and velocity commands. This works for statics and in some cases also dynamic obstacles.&lt;/p&gt;

&lt;h3 id=&quot;gap-finding-intuition&quot;&gt;Gap finding intuition&lt;/h3&gt;
&lt;p&gt;As the name of algorithm suggests, the car should be able to find a widest gap in the presence of immediate obstacles and drive into this gap to avoid any immediate collision. Let’s say my lidar is reporting following array of distances [0.2, 6.2, 6.0, 7.0, inf, 3.0, inf, 3.0, inf, 8.0, 1.0, 3.0] you could simply choose the farthest distance obstacle and drive towards it, however this may not be possible as the car would have to pass between too obstacles which range between them is not enough for car to fit in ( there is enough room in between than - this can be computed by computing length of the arc given the angular measurements), or the angle between these obstacles is not physically possible for car to turn - as in my array is case of 7th element. To approach this problem let’s define what a gap is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The gap is series of at least n consecutive hits that pass some distance threshold t.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we would apply this definition to the array given above with n = 3 and t = 5.0 we would get only one gap: [6.2, 6.0, 7.0, inf]. So we would steer towards the center of this gap.&lt;/p&gt;

&lt;p&gt;Problems with this approach is that purely following the deepest gap allows the car pass between the obstacles as the car might not fit in between them. To address this challenge we can use approach use in all of robotics.&lt;/p&gt;

&lt;h4 id=&quot;point-robot-approach&quot;&gt;Point Robot Approach&lt;/h4&gt;
&lt;p&gt;We can assume that the robot is circular. With this assumption we can inflate the  the obstacles with the radius of robot. So if I want the robot go between the obstacles I can always represent the robot as point object and still assure that the robot will fit between the two obstacles as I’m clearing the obstacles by at least the radius of robot itself. This is better dhown on figure bellow (source: &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/L14-Follow-the-gap.pdf&quot;&gt;UV lecture follow the gap&lt;/a&gt;):
&lt;img src=&quot;/assets/PRA.png&quot; alt=&quot;PRA&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;disparity&quot;&gt;Disparity&lt;/h4&gt;
&lt;p&gt;Looking trough the lidar readings for consecutive readings that differ by an amount over some threshold, than we mask these readings  to appear closer to the lidar. The reminding points which are farther than the mask are the actual points where the car can fit and drive trough. This concept is nicely visualised on the following figure from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/L14-Follow-the-gap.pdf&quot;&gt;UV lecture follow the gap&lt;/a&gt;
&lt;img src=&quot;/assets/ftg1.png&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;follow-the-gap-algorithm&quot;&gt;Follow the gap algorithm&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;step 1:
Find the nearest LIDAR point and pit safety bubble around it of radius rb&lt;/li&gt;
  &lt;li&gt;step 2:
Set all points inside bubble to distance 0. This can be achieved by computing the length of the arc and than determine which points falls into the radius of buble rb. All of these points are than set to 0.&lt;/li&gt;
  &lt;li&gt;step 3:
Find maximum sequence of consecutive non-zeros among the free-space points. This is the maximum gap where the car can drive.&lt;/li&gt;
  &lt;li&gt;step 4:
Find the best point among this maximum sequence from previous step.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make this as fast as possible, I should be looking from farther ranges for these obstacles, because doing sharp turns results in slower velocity, which can be achieved by turning earlier in less sharp angle.&lt;/p&gt;

&lt;h4 id=&quot;disadvantages&quot;&gt;Disadvantages&lt;/h4&gt;
&lt;p&gt;There is risk of doing U turns if he car is going too fast and it has to do 90 degrees turn which it can’t currently see as at particular point it will see the point on the opposite side of track, rather than the turn itself. This is shown on the figure bellow for better intuition.
&lt;img src=&quot;/assets/ftg_fail.png&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is also shown in the nice follow the gap practical demonstration in video bellow.&lt;/p&gt;
&lt;iframe width=&quot;700&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/ctTJHueaTcY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;f1tenth-lab4-assignment&quot;&gt;F1Tenth Lab4 assignment&lt;/h2&gt;
&lt;p&gt;My implementation of FtG algorithm consists of these steps:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Process each LiDAR scan by considering only field of view of 90 (45 to left and 45 to right as the 0th degree is exactly infront of the car) degrees, setting each value to the running mean of window 7 by convolution and clipping all values over 3m to 3m&lt;/li&gt;
  &lt;li&gt;Find the closest obstacle and set all points in bubble around it in certain radius 0. This bubble is computed by computing arc of proportion of radius and closest point and computing all angles inside this bubble. Ranges corresponding to the indexes of angles which are in the bubble are than set to 0.&lt;/li&gt;
  &lt;li&gt;Finding the max gap by finding the longest non-zero consecutive values in the indexes of the processed lidar scan&lt;/li&gt;
  &lt;li&gt;Finding the deepest possible scan and steer in the direction of it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My final package as required in the task is available bellow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/smejkka3/smejkka3.github.io/raw/master/assets/karel_lab4.zip&quot;&gt;karel_lab4.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;txt with video links of the assignment included in the zipfile.&lt;/p&gt;

&lt;iframe width=&quot;800&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/pKxiRvM4X6U&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name></name></author><summary type="html">Addressing the shortcomings of wallfollowing and implementing Follow the Gap algorithm.</summary></entry><entry><title type="html">Wallfollowing</title><link href="http://localhost:4000/2020/11/06/wallfollowing.html" rel="alternate" type="text/html" title="Wallfollowing" /><published>2020-11-06T00:00:00+01:00</published><updated>2020-11-06T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/06/wallfollowing</id><content type="html" xml:base="http://localhost:4000/2020/11/06/wallfollowing.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Lecture 3 as prerequisite for wallfollowing algorithm covers one of the last ROS introductory topic as that are Rigid Body Transformation.
Coordinate frames in ros such as map frame, lidar frame and other sensors provide informations specific to that sensor. Coordinate frame is
set of 3 orthogonal axes for X,Y and Z direction with a position where this frame is placed. Any position of word, robot, etc. only makes sense
when we defined the frame in which we are describing the pose. The direction of X,Y, Z axes is defined by right-handed rule, where index finger points in
X direction, middle finger in y direction and thumb in Z direction. Let’s look more into transformations and frames.&lt;/p&gt;

&lt;h2 id=&quot;transfomations-and-frames&quot;&gt;Transfomations and Frames&lt;/h2&gt;

&lt;p&gt;Why do we need transformations between different sensors? Each time I get data in sensor frame (for specific sensor) which I want to transform in some unified way.
For example we can have frame of reference of lidar which we can transform into the frame of reference of robot, which is the center of the rear axle. As example in
the last assignment we only take in account the frame of reference of lidar and stopped the car based on position of the lidar. However car have certain length and lidar
can be place in the center of the car. Therefore the correct way of stopping before collision should be calculated from the front of the car (or edge of the car), not from the
position of the lidar. Between frames there will exist transformation that convert measuring from one frame to another.&lt;/p&gt;

&lt;h2 id=&quot;reference-frames-on-f1tenth-car&quot;&gt;Reference Frames on F1Tenth car&lt;/h2&gt;

&lt;h4 id=&quot;map&quot;&gt;map&lt;/h4&gt;
&lt;p&gt;The origin set by the user and every other transformation can be measured with respect to the map. Represent the environment where the car will be racing. Can be place arbitrary and typically never moves after its placed.&lt;/p&gt;

&lt;h4 id=&quot;base_link&quot;&gt;base_link&lt;/h4&gt;
&lt;p&gt;Positioned at center of the rear axle of the car. Moves with the car relative to the map frame.&lt;/p&gt;

&lt;h4 id=&quot;lidar&quot;&gt;lidar&lt;/h4&gt;
&lt;p&gt;The frame of reference where lidar scan measurements are taken. Moves with the car relative to the map frame.&lt;/p&gt;

&lt;h4 id=&quot;odom&quot;&gt;odom&lt;/h4&gt;
&lt;p&gt;Typically required by ROS. Usually the initial position of the robot in the map before everything began. Fixed relative to the map.&lt;/p&gt;

&lt;h3 id=&quot;rigid-body-transfomation&quot;&gt;Rigid Body Transfomation&lt;/h3&gt;

&lt;p&gt;How to transform data and position between one frame and another. As on figure bellow, we want the coordinate of point p in frame 2, given the coordinate of point p in frame 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/frames.png&quot; alt=&quot;frames&quot; /&gt;
2 diffrent coordinate frames and point p from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf&quot;&gt;UV F1/10 course&lt;/a&gt;
  Steps:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Overlap both frames so their origins are at the same point. The reason is that we want to compute how is second frame of reference rotated. (apply rotation)&lt;/li&gt;
  &lt;li&gt;Describe the unit vectors of the second frame of reference in the terms of unit vectors of first frame of reference. As on picture bellow.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/frames2.png&quot; alt=&quot;frames2&quot; /&gt;
Overlapped frames of reference from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf&quot;&gt;UV F1/10 course&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;x&lt;sub&gt;2&lt;/sub&gt; = R&lt;sub&gt;11&lt;/sub&gt;x&lt;sub&gt;1&lt;/sub&gt; + R&lt;sub&gt;21&lt;/sub&gt; y&lt;sub&gt;1&lt;/sub&gt;&lt;br /&gt;
y&lt;sub&gt;2&lt;/sub&gt; = R&lt;sub&gt;12&lt;/sub&gt;x&lt;sub&gt;1&lt;/sub&gt; + R&lt;sub&gt;22&lt;/sub&gt; y&lt;sub&gt;1&lt;/sub&gt;&lt;br /&gt;
The formula above describes the units vector of new frame of referece as a linear combination of the original frame of reference. (considering only X,Y axis - 2D problem, no Z).
The formula can be also rewritten as matrix:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rotation_matrix.png&quot; alt=&quot;rotation_m1&quot; /&gt;
Rotation matrix from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf&quot;&gt;UV F1/10 course&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also we define θ as angle between x&lt;sub&gt;1&lt;/sub&gt; and x&lt;sub&gt;2&lt;/sub&gt; as shown on figure bellow. This angle tells us how rotated this frame is.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/theta.png&quot; alt=&quot;theta&quot; /&gt;
Overlapped frames of reference with angle theta from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf&quot;&gt;UV F1/10 course&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To get the coefficients R&lt;sub&gt;11&lt;/sub&gt;, R&lt;sub&gt;21&lt;/sub&gt;, R&lt;sub&gt;12&lt;/sub&gt;, R&lt;sub&gt;22&lt;/sub&gt;, along x&lt;sub&gt;2&lt;/sub&gt; unit vector direction, contains the cos unit component of x&lt;sub&gt;1&lt;/sub&gt;
unit vector as well as sin component of y&lt;sub&gt;1&lt;/sub&gt; unit vector. Thefore  x&lt;sub&gt;2&lt;/sub&gt; and similary y&lt;sub&gt;2&lt;/sub&gt; can be written as:&lt;/p&gt;

&lt;p&gt;x&lt;sub&gt;2&lt;/sub&gt; = cos(θ)x&lt;sub&gt;1&lt;/sub&gt; + sin(θ)y&lt;sub&gt;1&lt;/sub&gt;&lt;br /&gt;
y&lt;sub&gt;2&lt;/sub&gt; = -sin(θ)x&lt;sub&gt;1&lt;/sub&gt; + cos(θ)y&lt;sub&gt;1&lt;/sub&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Therefore I get:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rotation_matrix2.png&quot; alt=&quot;rotation_m&quot; /&gt;
Rotation matrix from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf&quot;&gt;UV F1/10 course&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With this we are able to represent unit vectors in the new frame of reference in terms of unit vectors of original frame of reference. Similarly we can express the position of point P in the new frame of reference using the rotation matrix as shown on formulas bellow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rotation_p.png&quot; alt=&quot;rotation&quot; /&gt;
Rotation matrix of point p from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf&quot;&gt;UV F1/10 course&lt;/a&gt;.
We can’t forget to apply the translation as well.&lt;/p&gt;

&lt;h4 id=&quot;ros-tftf2-package&quot;&gt;ROS tf/tf2 package&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;http://wiki.ros.org/tf2&quot;&gt;ROS tf2 package&lt;/a&gt; lets you keep track of multiple coordinate frames over time. And transform points/poses between two coordinates. It broadcast this across ROS so any node can subscribe to it. To get more insight how it works it is very useful to try &lt;a href=&quot;http://wiki.ros.org/tf2/Tutorials&quot;&gt;TF2 Tutorial&lt;/a&gt;, older &lt;a href=&quot;http://wiki.ros.org/tf/Tutorials&quot;&gt;TF Tutorial&lt;/a&gt; and &lt;a href=&quot;http://wiki.ros.org/tf2/Migration&quot;&gt;TF2 Migration&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;wallfollowing&quot;&gt;Wallfollowing&lt;/h4&gt;
&lt;p&gt;The idea is we are trying to compute the error between the future position of the car instead of current error because of constant movement of the car.
By minimising the future distance from the wall with respect to the optimal trajectory we are able to follow the wall.
Lets visualise the algorithm equations with the help from figures bellow. All of them taken from the &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/Wall_Following.pdf&quot;&gt;University of Virginia F1/10 Course from
Madhur Behl&lt;/a&gt;.
&lt;img src=&quot;/assets/wallfollowing1.png&quot; alt=&quot;wallfollowing1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/wallfollowing2.png&quot; alt=&quot;wallfollowing2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/wallfollowing3.png&quot; alt=&quot;wallfollowing3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If at any given point my car is at point A, the distance to the wall is B, but as already mentioned, I’m not trying to compute the error between B and the desired trajectory of the car visualised on the last figure as vertical green lines. You must project the car forward based on the velocity of the car and minimise the future distance to the wall with respect to the desired trajectory (CD in the case of figure). Using trigonometry the angle α can be computed knowing distances a and b which are the projected distances to the wall by lidar and corresponding angle between a and b called θ. Knowing the angle α I’m able to compute the current distance to the wall as well as future distance CD.&lt;/p&gt;

&lt;h4 id=&quot;pid&quot;&gt;PID&lt;/h4&gt;

&lt;p&gt;Using CD I can compute the error which is just difference between desired trajectory and CD. This error is important because we need it to setup the proportional and derivative controller to correct the steering angle of the car in the PID manner with respect to error.
 Formula for PID in the figure bellow (again from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/Wall_Following.pdf&quot;&gt;UV wallfollowing lecture&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/pid.png&quot; alt=&quot;pid&quot; /&gt;&lt;/p&gt;

&lt;p&gt;V&lt;sub&gt;θ&lt;/sub&gt; being correction in steering, K&lt;sub&gt;p&lt;/sub&gt; is proportion of my error  computed above as desired trajectory - CD. K&lt;sub&gt;d&lt;/sub&gt; is derivative gain and the rate of change of error &lt;sup&gt;de(t)&lt;/sup&gt;⁄&lt;sub&gt;dt&lt;/sub&gt;. This can be simplified by computing previous error - current error. Once I have the error correction in steering, I can update the steering angle angle with the correction. If the PID constants are well tuned, the steering angle should correct itself based on how far the car is from desired trajectory.&lt;/p&gt;

&lt;h2 id=&quot;f1tenth-lab3-assignment&quot;&gt;F1Tenth Lab3 assignment&lt;/h2&gt;
&lt;p&gt;Moving on to my implementation of the WallFollow node. The algorithm implemented consists of these steps:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Step 1. Obtain two laser scans (distances) a and b, with b taken at 0 degrees and a at an angle theta (0 &amp;lt; theta =&amp;lt; 70),
    &lt;ul&gt;
      &lt;li&gt;the 0th angle corresponds to the front of the f1tenth car and the positive angle direction corresponds to the left of the car, therefore choice of 0 degree angle and in my case 60 degree angle corresponds to the left wall following.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Step 2. Use the distances a and b to calculate the angle alpha between the car’s x- axis and the left wall and use alpha to find the current distance D_t to the car,
    &lt;ul&gt;
      &lt;li&gt;all of these are calculated using the formulas described in the wallfollowing section.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Step 3. And than alpha and D_t to find the estimated future distance D_t1 to the wall&lt;/li&gt;
  &lt;li&gt;Step 4. Run D_t1 trough the PID algorithm described above ( in assignment) to get a steering angle&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I was experimenting for a long time with different values of K&lt;sub&gt;p&lt;/sub&gt;,K&lt;sub&gt;d&lt;/sub&gt; and K&lt;sub&gt;i&lt;/sub&gt;. Using this I was able to find get a more insight what each of these constant is doing. I’m not 100% sure about, but I think that lowering K&lt;sub&gt;p&lt;/sub&gt; makes the car less responsive to the error magnitude, as increasing this constant makes car move move with a twisting motion, K&lt;sub&gt;d&lt;/sub&gt; seems to react faster to the growing error but it wasn’t completely clear to me from simulation. K&lt;sub&gt;i&lt;/sub&gt; didn’t seem to do much even at higher values, but according to the research it should make the car more sensitive to error.&lt;/p&gt;

&lt;p&gt;I’ve found that setting the right theta had probably biggest impact on the car ability to follow the wall without bouncing to the corners. Also one additional thing while filtering nan and inf values from the scan reading, I also removed the readings that are higher angle that the lidar sensor on real car is able to read, which is apparently 270 degrees. The car in the simulation has no problem reading in 360 dergees so there are none infinite or nan values in simulation.&lt;/p&gt;

&lt;p&gt;My final package as required in the task is available bellow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/smejkka3/smejkka3.github.io/raw/master/assets/karel_lab3.zip&quot;&gt;karel_lab3.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;txt with video links of the assignment included in the zipfile.&lt;/p&gt;

&lt;iframe width=&quot;800&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/5nLtlszkRvI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name></name></author><summary type="html">Fist autonomous lap of my F1Tenth car using wallfollowing algorithm.</summary></entry><entry><title type="html">Automatic Emergency Braking</title><link href="http://localhost:4000/2020/11/02/aeb.html" rel="alternate" type="text/html" title="Automatic Emergency Braking" /><published>2020-11-02T00:00:00+01:00</published><updated>2020-11-02T00:00:00+01:00</updated><id>http://localhost:4000/2020/11/02/aeb</id><content type="html" xml:base="http://localhost:4000/2020/11/02/aeb.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The next assignment after the ROS introductory is create a node which uses laser_scan data and prevents the car from collision
into nearby objects. This is achieved by computing TTC (Time To Collision) using before mentioned LaserScan message. This assignment
checks more into the depth the ability work with publishers and subscribe and forces me to explore more Odometry message and
AckermannDriveStamped message because both of them are used to prevent the collision. But first let’s look at some theory.&lt;/p&gt;

&lt;h2 id=&quot;aeb-automatic-emergency-braking&quot;&gt;AEB (Automatic Emergency Braking)&lt;/h2&gt;

&lt;p&gt;AEB is simply forcing stop before expected collision with nearby object. It is a safety reaction of the car based on the
information it is gathering from its sensors. It is simple binary classification problem which answers question break/not break.&lt;/p&gt;

&lt;h4 id=&quot;failures&quot;&gt;Failures&lt;/h4&gt;

&lt;p&gt;The big problem in AEB is false negative. This means the car won’t stop when it is about to collide with an obstacle. As you can imagine no one will deploy a car which can’t avoid collision or even can kill people. Another, but not as serious problem are false positives, when car stops randomly when it should not and there is no danger of colliding.&lt;/p&gt;

&lt;h4 id=&quot;ttc-time-to-collision&quot;&gt;TTC (Time to Collision)&lt;/h4&gt;

&lt;p&gt;Let’s explain how TTC is calculated:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/TTC_formula.png&quot; /&gt;
  Time to Collision formula from &lt;a href=&quot;https://f1tenth.org/learn.html&quot;&gt;lecture 2&lt;/a&gt; of F1/10 course&lt;/p&gt;

&lt;p&gt;where denominator that’s time derivative of range between vehicle and given obstacle, also called as “range rate” is defined as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/range_rate.png&quot; /&gt;
  Formula  for range-rate from &lt;a href=&quot;https://f1tenth.org/learn.html&quot;&gt;lecture 2&lt;/a&gt; of F1/10 course&lt;/p&gt;

&lt;h4 id=&quot;lidar&quot;&gt;Lidar&lt;/h4&gt;

&lt;p&gt;Although I have worked with lidar in previous assignment, it worths explaining the &lt;a href=&quot;http://docs.ros.org/en/api/sensor_msgs/html/msg/LaserScan.htmlLaserScan&quot;&gt;LaserScan message data fields&lt;/a&gt;
I will have to use in this assignment. Based on the TTC formula above, I’m gonna need &lt;i&gt;float32[] ranges&lt;/i&gt; to calculate distance between obstacle and the car, which is numerator in the the TTC formula.
For denominator (range-rate) I have to use all &lt;i&gt;float32 angle_min, float32 angle_max and float32 angle_increment&lt;/i&gt;. That’s because for range-rate computation I need cosine of each beam angle (with these 3 variables
I can get every single angle of my LaserScan data). Also as mentioned I need to take max of given range-rate of given beam and 0 so meaning if range-range for specific beam is less than 0, assign it 0. In both of my nodes (Python and C++) to avoid division by 0 I set this value very close
to 0. I was trying to remove 0 from denominator completely together with numerator at the same index (same LaserScan angle), however this caused lot of false negatives.&lt;/p&gt;

&lt;p&gt;Another message I need is &lt;i&gt;Odometry&lt;/i&gt;, specifically &lt;i&gt;geometry_msgs/TwistWithCovariance twist.linear.x&lt;/i&gt;, which is linear speed of my car, used
again in the range-rate calculation.&lt;/p&gt;

&lt;h2 id=&quot;f1tenth-lab2-assignment&quot;&gt;F1Tenth Lab2 assignment&lt;/h2&gt;
&lt;p&gt;TTC threshold for braking was set to 0.3 according to my own experimentation on the RViz simulator. When TTC is less than this threshold, the brake is applied.
My final package as required in the task is available bellow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/smejkka3/smejkka3.github.io/raw/master/assets/karel_lab2.zip&quot;&gt;karel_lab2.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;txt with video links of the assignment included in the zipfile.&lt;/p&gt;
&lt;iframe width=&quot;800&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/LXWpBoFb4nk&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;iframe width=&quot;800&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/zna-dPAIdUQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;
&lt;/p&gt;</content><author><name></name></author><summary type="html">Exploring how to stop car autonomously before it hits an obstacle.</summary></entry><entry><title type="html">Introduction to Robotic Operating System (ROS)</title><link href="http://localhost:4000/2020/10/23/Introduction.html" rel="alternate" type="text/html" title="Introduction to Robotic Operating System (ROS)" /><published>2020-10-23T00:00:00+02:00</published><updated>2020-10-23T00:00:00+02:00</updated><id>http://localhost:4000/2020/10/23/Introduction</id><content type="html" xml:base="http://localhost:4000/2020/10/23/Introduction.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;To start with F1Tenth, the first lecture covers mostly overview of the whole curse and a brief introduction of autonomous racing. The point of the lecture was mostly to motivate students and set an outline of what to expect during the course.&lt;/p&gt;

&lt;p&gt;After watching the first lecture the next content in line was the first tutorial. The slides on the website only briefly cover the F1Tenth simulator how to install it and how to manually drive the car inside of the simulator using keyboard. So I used lecture videos from University of Virginia by Prof. Madhur Behl available &lt;a href=&quot;https://www.youtube.com/playlist?list=PL868twsx7OjddCq3az74hu6pVsuJJzXvP&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/&quot;&gt;here&lt;/a&gt; as well as watched the first 5 videos all the way up to [F1/10 Lectures] Online ROS F1/10 Simulator to cover all the basics of ROS, which are later needed in the Lab1 Assignment. I’m gonna list bellow some of the basic points I should remember from these lectures.&lt;/p&gt;

&lt;h2 id=&quot;ros-basics&quot;&gt;ROS basics&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;http://wiki.ros.org/&quot;&gt;ROS wiki&lt;/a&gt; already has great tutorials to use when learning basics of ROS, however I’m gonna mention some of the main points from the first lecture. It was focused on the ROS as middleware which manages communication between different parts of the autonomous car. Here are the some of the basics components of ROS:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/Nodes&quot;&gt;Nodes&lt;/a&gt; Programs with specific functionality, that runs as a single process and they communicate with other nodes using topics and messages. A node is written using the client library, the main ones are roscpp for C++ and rospy for Python. There are also some experimental client libraries such as R and Java.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/Master&quot;&gt;ROS Master&lt;/a&gt; is special node which runs always and it doesn’t have to be written by user. It allows nodes to be able to exchange message among each other. ROS Master always has to be running. To start the ROS Master Node there is a command
        &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; roscore
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/Topics&quot;&gt;Topics&lt;/a&gt; are channels over which nodes exchange messages. Nodes can subscribe to or publish to a topic. Topics has many-to-many relationship, so there can be multiple publishers and multiple subscribers on one topic and each topic has only one type of message.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/Messages&quot;&gt;Messages&lt;/a&gt; are strongly-typed data structures for a topic.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/Packages&quot;&gt;Packages&lt;/a&gt; are part of software, which contains one or more nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The ROS Publishers and Subscribers, Messages and topic concept is very well described at &lt;a href=&quot;https://www.mathworks.com/help/ros/ug/exchange-data-with-ros-publishers-and-subscribers.html&quot;&gt;mathworks.com&lt;/a&gt; with the following image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.mathworks.com/help/examples/ros/win64/ExchangeDataWithROSPublishersAndSubscribersExample_01.png&quot; alt=&quot;mathworks&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next thing to try for me was to follow the turtlesim tutorial, which is basically “hello world” of ROS. The tutorial is available at &lt;a href=&quot;http://wiki.ros.org/turtlesim/Tutorials&quot;&gt;ROS wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next lecture was about ROS filesystem. Although very important topic to know to successfully work in ROS, it is better to try it on your own inside turtlesim or your own ROS project. So what is &lt;a href=&quot;http://wiki.ros.org/ROS/Tutorials/NavigatingTheFilesystem&quot;&gt;ROS filesystem&lt;/a&gt;? It centralises the build process of a project, while at the same time provide enough flexibility and tooling to decentralise its dependencies. The main points of the lecture were about &lt;a href=&quot;http://wiki.ros.org/catkin&quot;&gt;catkin&lt;/a&gt;, &lt;a href=&quot;http://wiki.ros.org/catkin/commands/catkin_make&quot;&gt;cakin_make&lt;/a&gt; command and most importantly about &lt;a href=&quot;http://wiki.ros.org/catkin/package.xml&quot;&gt;package.xml&lt;/a&gt; and &lt;a href=&quot;http://wiki.ros.org/catkin/CMakeLists.txt&quot;&gt;CMakeLists.txt&lt;/a&gt; file inside ROS package.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/catkin&quot;&gt;Catkin&lt;/a&gt;: Is the build system of ROS which generates executables nad libraries. It is based on CMake from C programming language and it extends it with ROS specific features. It also makes the package more standard compliant, and thus reusable by other programmers. In the following figure is shown how the catkin workspace should be organised (taken from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/L03.pdf&quot;&gt;UV F1/10 lecture&lt;/a&gt;).
&lt;img src=&quot;/assets/catkin_ws.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/catkin/package.xml&quot;&gt;Package.xml&lt;/a&gt;: Contains the meta information of a package such as name, description, version, license and dependencies.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/catkin/CMakeLists.txt&quot;&gt;CMakeLists.txt&lt;/a&gt;: The main CMake file to build the package and calls catkin-specific functions. Example of how should very basic  * CMakeLists.txt look look like is in the following figure (taken from &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/L03.pdf&quot;&gt;UV F1/10 lecture&lt;/a&gt;):
CMakeLists.txt
&lt;img src=&quot;/assets/CMake.png&amp;quot;&quot; alt=&quot;cmake&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moving on to the next video and Lecture 3 from the University of Virginia, which talks about one of the most important concepts in ROS and that’s Publishers and Subscribers. This lecture starts with example of &lt;a href=&quot;http://wiki.ros.org/rospy&quot;&gt;rospy client library&lt;/a&gt;, explaining initialisation of ROS Node with&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rospy.init_node&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'my_node_name'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rospy.init_node&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'my_node_name'&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;anonymous&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;True&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This tells ROS that this code is ROS node with the name my_node_name and this name must be unique. The anonymous=True parameter create unique name for you adding unique id to the end of the node name. To not kill the node immediately after one run using&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rospy.spin&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to achieve this by consuming some CPU cycles.&lt;/p&gt;

&lt;h4 id=&quot;publisher&quot;&gt;Publisher&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;http://wiki.ros.org/ROS/Tutorials/WritingPublisherSubscriber%28c%2B%2B%29&quot;&gt;ROS Publisher&lt;/a&gt; is a node which will continually broadcast a message.
&lt;img src=&quot;/assets/publisher.png&quot; alt=&quot;publisher&quot; /&gt;
Example of Publisher Node with explanation of what each line is doing, ref: &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/L04-compressed.pdf&quot;&gt;UV F1/10 lecture&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;subscriber&quot;&gt;Subscriber&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;http://wiki.ros.org/ROS/Tutorials/WritingPublisherSubscriber%28c%2B%2B%29&quot;&gt;ROS Subscriber&lt;/a&gt; tells ROS that you want to receive messages on a given topic.
&lt;img src=&quot;/assets/subscriber.png&quot; alt=&quot;subscriber&quot; /&gt;
Example of Subscriber Node with explanation of what each line is doing, ref: UV F1/10 lecture
The next video is hands on ROS commands and exploring the workspace from terminal available here. It is great hands-on experience of all the lectures before and worth trying it and following along.&lt;/p&gt;

&lt;h2 id=&quot;autoturtle-assignment&quot;&gt;Autoturtle assignment&lt;/h2&gt;

&lt;p&gt;Although I completed &lt;a href=&quot;https://f1tenth-coursekit.readthedocs.io/en/stable/assignments/labs/lab1.html#lab-1-introduction-to-ros&quot;&gt;Lab1 of F1Tenth&lt;/a&gt; already, I decided to do the &lt;a href=&quot;https://linklab-uva.github.io/autonomousracing/assets/files/A01.pdf&quot;&gt;first assignment&lt;/a&gt; from the F1/10 course from University of Virginia as well to gain very good base knowledge of ROS. There are 4 tasks in total. The first one is to create node swim_shool.py where the turtle from turtlesim tutorial draw figure 8 by doing movement specified by linear and angular velocity defined by user. I’m not uploading the code as it is university course credited assignment but my results are shown in gif bellow. My first idea was to create subscriber in the node which is getting the pose of the turtle and when the pose.theta is very close to 0 (means horizontal to X axis) multiply angular velocity of turtle by -1, which makes the turtle turn the other side to which is turtle currently rotating. However due the slight inaccuracies of the pose.theta and rospy.rate the &lt;a href=&quot;http://docs.ros.org/en/melodic/api/turtlesim/html/msg/Pose.html&quot;&gt;pose.theta&lt;/a&gt; never was exactly 0 or its absolute value was not close enough to accurately follow the 8 figure each round. I partially solved this by increasing queue size and rospy.rate to 1000hz to increase rate of loop which gave me more pose.theta values, however this solution still was not accurate enough. What worked great is to calculate the radius of the circle from the linear and angular velocities inputted by user. Using this radius I could calculate circumference of the circle the turtle is going to draw as well as at the same time computing the distance which turtle travelled at each run of the loop and once the turtle crossed the distance of circumference of the circle multiply angular velocity of turtle by -1 and combine this approach with the pose.theta checking (as the circumference check was not accurate enough and the turtle turned slightly before finishing the complete circle) mention above with slightly less accuracy. Combining these two condition made my turtle not skip the turns after finishing drawing the complete circle.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/swim_school1.gif&quot; alt=&quot;swim_school1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/swim_school2.gif&quot; alt=&quot;swim_school2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The second task random_swim_shool.py was very similar to the previous one. The only difference is that the initial position of the turtle should be random as well as linear and angular velocity of the turtle. My results shown again in gifs bellow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/random_swim_school1.gif&quot; alt=&quot;random_swim_school1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/random_swim_school2.gif&quot; alt=&quot;random_swim_school2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the third task back_to_sqaure_one.py the task was again to draw a shape by moving turtle, however this time it was square shape with the lower left corner at position (1,1). Side length of the square should be taken from user’s input in the range between 1 to 5.
&lt;img src=&quot;/assets/back_to_sqaure_one.gif&quot; alt=&quot;back_to_sqaure_one&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The last problem was to swim to position defined by a user.
&lt;img src=&quot;/assets/swim_to_goal.gif&quot; alt=&quot;swim_to_goal&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;f1tenth-lab1-assignment&quot;&gt;F1Tenth Lab1 assignment&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://f1tenth-coursekit.readthedocs.io/en/stable/assignments/labs/lab1.html#lab-1-introduction-to-ros&quot;&gt;First Lab&lt;/a&gt; of the F1Tenth was quite straight forward. The main goal is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;to understand directory structure and framework of ROS&lt;/li&gt;
  &lt;li&gt;to understand and be able to implement simple subscribers and publishers&lt;/li&gt;
  &lt;li&gt;to understand and be able to implement messages&lt;/li&gt;
  &lt;li&gt;to understand what exactly is in CMakeLists.txt and package.xml&lt;/li&gt;
  &lt;li&gt;to understand package dependencies&lt;/li&gt;
  &lt;li&gt;be able to write and to understand launch files&lt;/li&gt;
  &lt;li&gt;to work with RViz&lt;/li&gt;
  &lt;li&gt;to understand and work Bag files.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The goal is to implement ROS package with node (either in C++, Python or both) and launch file and custom message, which subscribes to the laser_scan topic and publishes distance to the closest obstacle, farthest obstacle and both of these in one topic. I decided to do the lab in both C++ and Python as I want to practice C++ for later when it will be useful for computer vision as python is known for being quite slow. I didn’t encounter some major problems, only experienced little delay while coding the C++ part as I had to iterate trough the vector of laser_scan message ranges and as it was already more than a year sice I did major project in C++ I had to go back to basic tutorials on iterators in C++. My final package as required in the task is available bellow.&lt;/p&gt;

&lt;p&gt;PDF with theory part of the assignment included in the &lt;a href=&quot;https://github.com/smejkka3/smejkka3.github.io/raw/master/assets/karel_lab1.zip&quot;&gt;zipfile&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">My first steps into F1Tenth. Mostly just ROS introduction.</summary></entry></feed>