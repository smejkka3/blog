I"•<h2 id="introduction">Introduction</h2>

<p>Lecture 3 as prerequisite for wallfollowing algorithm covers one of the last ROS introductory topic as that are Rigid Body Transformation.
Coordinate frames in ros such as map frame, lidar frame and other sensors provide informations specific to that sensor. Coordinate frame is
set of 3 orthogonal axes for X,Y and Z direction with a position where this frame is placed. Any position of word, robot, etc. only makes sense
when we defined the frame in which we are describing the pose. The direction of X,Y, Z axes is defined by right-handed rule, where index finger points in
X direction, middle finger in y direction and thumb in Z direction. Letâ€™s look more into transformations and frames.</p>

<h2 id="transfomations-and-frames">Transfomations and Frames</h2>

<p>Why do we need transformations between different sensors? Each time I get data in sensor frame (for specific sensor) which I want to transform in some unified way.
For example we can have frame of reference of lidar which we can transform into the frame of reference of robot, which is the center of the rear axle. As example in
the last assignment we only take in account the frame of reference of lidar and stopped the car based on position of the lidar. However car have certain length and lidar
can be place in the center of the car. Therefore the correct way of stopping before collision should be calculated from the front of the car (or edge of the car), not from the
position of the lidar. Between frames there will exist transformation that convert measuring from one frame to another.</p>

<h2 id="reference-frames-on-f1tenth-car">Reference Frames on F1Tenth car</h2>

<h4 id="map">map</h4>
<p>The origin set by the user and every other transformation can be measured with respect to the map. Represent the environment where the car will be racing. Can be place arbitrary and typically never moves after its placed.</p>

<h4 id="base_link">base_link</h4>
<p>Positioned at center of the rear axle of the car. Moves with the car relative to the map frame.</p>

<h4 id="lidar">lidar</h4>
<p>The frame of reference where lidar scan measurements are taken. Moves with the car relative to the map frame.</p>

<h4 id="odom">odom</h4>
<p>Typically required by ROS. Usually the initial position of the robot in the map before everything began. Fixed relative to the map.</p>

<h3 id="rigid-body-transfomation">Rigid Body Transfomation</h3>

<p>How to transform data and position between one frame and another. As on figure bellow, we want the coordinate of point p in frame 2, given the coordinate of point p in frame 1.</p>

<p><img src="/assets/frames.png" alt="frames" />
2 diffrent coordinate frames and point p from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a>
  Steps:</p>
<ul>
  <li>Overlap both frames so their origins are at the same point. The reason is that we want to compute how is second frame of reference rotated. (apply rotation)</li>
  <li>Describe the unit vectors of the second frame of reference in the terms of unit vectors of first frame of reference. As on picture bellow.</li>
</ul>

<p><img src="/assets/frames2.png" alt="frames2" />
Overlapped frames of reference from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a></p>

<p>x<sub>2</sub> = R<sub>11</sub>x<sub>1</sub> + R<sub>21</sub> y<sub>1</sub><br />
y<sub>2</sub> = R<sub>12</sub>x<sub>1</sub> + R<sub>22</sub> y<sub>1</sub><br />
The formula above describes the units vector of new frame of referece as a linear combination of the original frame of reference. (considering only X,Y axis - 2D problem, no Z).
The formula can be also rewritten as matrix:</p>

<p><img src="/assets/rotation_matrix.png" alt="rotation_m1" />
Rotation matrix from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a></p>

<p>Also we define Î¸ as angle between x<sub>1</sub> and x<sub>2</sub> as shown on figure bellow. This angle tells us how rotated this frame is.</p>

<p><img src="/assets/theta.png" alt="theta" />
Overlapped frames of reference with angle theta from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a></p>

<p>To get the coefficients R<sub>11</sub>, R<sub>21</sub>, R<sub>12</sub>, R<sub>22</sub>, along x<sub>2</sub> unit vector direction, contains the cos unit component of x<sub>1</sub>
unit vector as well as sin component of y<sub>1</sub> unit vector. Thefore  x<sub>2</sub> and similary y<sub>2</sub> can be written as:</p>

<p>x<sub>2</sub> = cos(Î¸)x<sub>1</sub> + sin(Î¸)y<sub>1</sub><br />
y<sub>2</sub> = -sin(Î¸)x<sub>1</sub> + cos(Î¸)y<sub>1</sub><br /></p>

<p>Therefore I get:</p>

<p><img src="/assets/rotation_matrix2.png" alt="rotation_m" />
Rotation matrix from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a></p>

<p>With this we are able to represent unit vectors in the new frame of reference in terms of unit vectors of original frame of reference. Similarly we can express the position of point P in the new frame of reference using the rotation matrix as shown on formulas bellow.</p>

<p><img src="/assets/rotation_p.png" alt="rotation" />
Rotation matrix of point p from <a href="https://linklab-uva.github.io/autonomousracing/assets/files/ROS-tf.pdf">UV F1/10 course</a>.
We canâ€™t forget to apply the translation as well.</p>

<h4 id="ros-tftf2-package">ROS tf/tf2 package</h4>
<p><a href="http://wiki.ros.org/tf2">ROS tf2 package</a> lets you keep track of multiple coordinate frames over time. And transform points/poses between two coordinates. It broadcast this across ROS so any node can subscribe to it. To get more insight how it works it is very useful to try <a href="http://wiki.ros.org/tf2/Tutorials">TF2 Tutorial</a>, older <a href="http://wiki.ros.org/tf/Tutorials">TF Tutorial</a> and <a href="http://wiki.ros.org/tf2/Migration">TF2 Migration</a>.</p>

<h4 id="wallfollowing">Wallfollowing</h4>
<p>The idea is we are trying to compute the error between the future position of the car instead of current error because of constant movement of the car.
By minimising the future distance from the wall with respect to the optimal trajectory we are able to follow the wall.
Lets visualise the algorithm equations with the help from figures bellow. All of them taken from the <a href="https://linklab-uva.github.io/autonomousracing/assets/files/Wall_Following.pdf">University of Virginia F1/10 Course from
Madhur Behl</a>.
<img src="/assets/wallfollowing1.png" alt="wallfollowing1" /></p>

<p><img src="/assets/wallfollowing2.png" alt="wallfollowing2" /></p>

<p><img src="/assets/wallfollowing3.png" alt="wallfollowing3" /></p>

<p>If at any given point my car is at point A, the distance to the wall is B, but as already mentioned, Iâ€™m not trying to compute the error between B and the desired trajectory of the car visualised on the last figure as vertical green lines. You must project the car forward based on the velocity of the car and minimise the future distance to the wall with respect to the desired trajectory (CD in the case of figure). Using trigonometry the angle Î± can be computed knowing distances a and b which are the projected distances to the wall by lidar and corresponding angle between a and b called Î¸. Knowing the angle Î± Iâ€™m able to compute the current distance to the wall as well as future distance CD.</p>

<h2 id="f1tenth-lab3-assignment">F1Tenth Lab3 assignment</h2>
<p>My final package as required in the task is available bellow.</p>

<p><a href="https://github.com/smejkka3/smejkka3.github.io/raw/master/assets/karel_lab3.zip">karel_lab3.zip</a></p>

<p>txt with video links of the assignment included in the zipfile.</p>

<iframe width="800" height="400" src="https://www.youtube.com/embed/5nLtlszkRvI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
:ET