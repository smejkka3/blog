I"F<h2 id="introduction">Introduction</h2>
<p>Moving away from reactive methods to different approach to autonomy and that’s utilising mapping information around the car. This method is called Simultaneous Localization and Mapping or for short SLAM. I’m going to describe some building blocks for SLAM such as Occupancy grid maps and also describe some of the basic SLAM algorithms out there and how they work. Localization is problem if we are given the map of environment we should figure out where we are located in the map. Mapping problem is complimentary to localization, knowing the pose of the car we need to build map of environment using the sensors available to us. SLAM is combination of these two to use these data to estimate trajectory of the robot and build the map at the same time. SLAM is way more beneficial over algorithms such as wallfollowing or follow the map because it allow us to plan longterm path of the robot and not only reacting to the momentary information.</p>

<h2 id="occupancy-grid-map">Occupancy Grid Map</h2>
<p>Occupancy can be though as binary random variable which can take 2 values: Free, Occupied.</p>

<p>Formal definition is:
m<sub>x,y</sub>:{free, occupied} -&gt; {0, 1}</p>

<p>Grid map is sequence of cell where each cell has occupancy variable, either free or occupied.</p>

<h4 id="occupancy-grid-mapping">Occupancy grid mapping:</h4>
<p>We want to assign a probability to every cell of the grid if the cell is free or occupied based on the robot’s data. To do this we use Bayesian filtering. Where we look at prior and recursively update the status of probability for each cell. To do so we need to use Bayes rule. We also need information from the measurements of the robot to tell us where is the obstacle located in the frame of reference of the robot. So the measurements for each cell can be reported either free or occupied, so we can think of this as a likelihood of observing certain measurement given what we know about the cells. To describe this formally:</p>

<p>Measurement model:
p(z|m<sub>x,y</sub>) where z~{0,1}
probability of z(measurement) conditioned upon what we know about the value of m(occupancy random variable) of the occupancy grid.</p>

<p>we can have these different measurement models only:
p(z = 1|m<sub>x,y</sub> = 1)-&gt; TRUE occupied measurement,
p(z = 0|m<sub>x,y</sub> = 1)-&gt; FALSE free measurement,
p(z = 1|m<sub>x,y</sub> = 0)  -&gt; FALSE occupied measurement,
p(z = 0|m<sub>x,y</sub> = 0)  -&gt; TRUE free measurement.</p>

<p>So taking all of these together into bayes rule:
<img src="/assets/bayes.png" alt="bayes" />
Given a occupancy grid (Prior map), we measure the data from the robot and we have a probability associated with observing 0 or 1 conditioned on state itself (measurement model) and we use this to update our knowledge of the occupancy of the grid cell or computing the posterior map ( the probability of cell being occupied of free conditioned upon what the measurement has reported ). This can be done by applying Bayes rule as shown in figure above (ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>).</p>

<p>But working with these probabilities can become mathematically inconvenient. Instead it is better to work with Odds.
<img src="/assets/odd1.png" alt="odd1" />
So given the formula above ( ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>) we can see that odd is probability of X happening divided by probability of X not happening so basically ratio of probabilities.
More specifically the Odd that a certain cell is occupied given z is a probability that the cell is occupied given z divided by probability that cell is not occupied given z as shown in the formula bellow ( ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>).
<img src="/assets/odd2.png" alt="odd2" />
By applying Bayes rule to the numerator:
<img src="/assets/bayes_odd.png" alt="bayes_odd" />
As well as by applying Bayes rule to the denominator:
<img src="/assets/bayes_odd2.png" alt="bayes_odd2" />
The evidence term p(z) can be eliminated. And on the resulting equation the log can be applied resulting in log in sum of two odds which results in the new log odd being equal to the previous log odd + log odd of the measurement which is easier to compute and which denotes the same thing as non-log version. In formulas shown below ( ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>):</p>

<p><img src="/assets/odd3.png" alt="odd3" /></p>

<p>So the measurement model in the log odd version will become:
<img src="/assets/measurementodd.png" alt="measurementodd" /></p>

<p>Do not forget that every grid cell is updated at each point in time. Also we haven’t explore where is the robot in the map. As well as the description above is considering only one single lidar beam scan. Let’s explore this further. If you haven’t check the previous post on ROS transformations, because it is needed to understand the next section.</p>

<h3 id="range-measurement-on-the-grid">Range measurement on the Grid</h3>
<p>Having map frame and base_link frame, we assume that we get measurement along primary x1 direction d distance from the abse link of the robot.At any given time we know the pose of the robot. We can use this information to find out that the reported occupied cell is d distance away from the base link of the robot and we can figure out coordinates of this point and express it in the global frame of reference using rigid body transformation equation shown bellow described in the previous post.
<img src="/assets/tf.png" alt="tf" />
All this was done in continuous space. To convert it to grid space we have to know the resolution of the grid r and than we can  simply put the point in grid i where i = ceil(x/r), of course because we have x<sub>1</sub> and x<sub>2</sub> we have to calculate both i<sub>1</sub> and i<sub>2</sub>. Of course the robot is giving us a lot of measurements at any given time, not only one. In this case we have to also account for α<sub>k</sub>, which stands for single lidar beam of the robot. All of what I described is much better shown on the image bellow ( ref <a href="https://linklab-uva.github.io/autonomousracing/assets/files/SLAM.pdf">UV SLAM lecture</a>).
<img src="/assets/tf2.png" alt="tf2" /></p>

<h2 id="google-cartographer">Google cartographer</h2>
<p>This algorithm is very effective in the part of SLAM called loop closure, it reduce computational requirements of loop closure.</p>

<h3 id="loop-closure">Loop closure</h3>
<p>We know that there are errors accumulating over time as the robot is updating the grid map measurement so it can happen that the map can start shifting from the actual word. If I revisit a place on map which I have visited before we can correct all this accumulated error. Basically we can overlap regions of map which are found to be the same based on this point of map where we know we have already been. Here is very nice video of showing this in action:</p>
<iframe width="800" height="443" src="https://www.youtube.com/embed/-EQAJOoRqEQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Moreover google cartographer is divided into two sub-system.</p>

<h4 id="local-slam">Local SLAM</h4>
<p>The job of local SLAM is generate good submaps. Submap is defined by how much data the robot received within a threshold. Submap must be small enough so the uncertainty of localization  is bellow the resolution of occupancy grid, so they are locally correct. However at the same time they have to be large enough to be different enough for the loop closure work correctly.</p>

<h4 id="global-slam">Global SLAM</h4>
<p>The job of local SLAM is to tie submaps consistently together as well as loop closure.</p>
:ET