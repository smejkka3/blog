I"I<p>Mentioned in post before, the goal of the project is to implement algorithm mentioned in the paper from <a href="https://www.robots.ox.ac.uk/~mobile/Papers/2015IJRR_wang.pdf">Wang et. al</a>. The paper focuses on creating framework that estimates and determines what is static background as well as movement of dynamic objects.</p>

<p>The algorithm itself can be divided into several parts. Firstly the new laser and odometry measurements coming from car need to be processed. The process of laser measurement consists of first cleaning the states which is mentioned very briefly in the paper and from my understanding, this means that all points from the previous measurement which are no longer in the LiDAR’s field of view, should be dropped. The authors do not provide any reasoning for this but my guess is that it’s due the easing some of the memory requirement as we do not essentially need to work with anything which we can’t see. One of our own modification during cleaning the states is also drop all points in the memory which are beyond certain threshold. This threshold is chosen to maximise the distance to which we want to detect dynamic object, but at the same time minimise the memory requirement as the assumption at the moment is that the whole framework needs to run in real-time to be usable, which can be hard to achieve with too many “memorised” points. This process is followed by forward propagation, where the motion part of all dynamic tracks according to an appropriate motion model is predicted. This motion model is required, due the main objective of the algorithm, to not have any assumption about the object class (therefore it’s expected motion pattern). This steps is followed by association and update of dynamic and static object measurements finalised by merging tracks which correspond to the same class (static or specific dynamic track). The basic idea of the algorithm is outlined in following figure
<img src="/assets/wangetal.png" alt="alg" /></p>

<p>As mentioned already the main benefit of this approach is that it allows us to track objects without having to know any informations about the object beforehand. Let’s describe each step more into the detail to understand more details about the algorithm.</p>

<h4 id="sensor-pose-prediction-on-odometry-measurement">Sensor pose prediction on odometry measurement</h4>
<p>Individual laser scan beams and a set of odometry measurements are used as input to the algorithm. Without odometry information, the system would not be able to distinguish static objects as all motion estimates are relative to the LiDAR sensor. Odometry information provides us with an estimate of speed $v$ of non-holonomic vehicle, mean angle $\theta$ of front wheels.</p>
<h4 id="dynamic-object-motion-prediction">Dynamic object motion prediction</h4>

<h4 id="hierarchical-data-association">Hierarchical data association</h4>

<h5 id="coarse-level-data-association">Coarse-level data association</h5>

<h5 id="fine-level-data-association">Fine-level data association</h5>

<h4 id="track-initialization-and-merging">Track initialization and merging</h4>
:ET